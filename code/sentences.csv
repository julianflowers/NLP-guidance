"","ID","sentence"
"1","Code.md_1","* You can do [LSA](.."
"2","Code.md_2","/LSA.md) or [LDA](.."
"3","Code.md_3","/LDA.md) in R using the `NLP-guidance."
"4","Code.md_4","Rproj` R project file * You can try some [pre-trained Neural Network generated vectors](.."
"5","Code.md_5","/NNmodels.md) in Python via the `Pretrained_vectors.ipynb` file To let you try out some of these ideas in practice, because I think that's a good way to learn."
"6","Code.md_6","The data we use for practicing these techniques is the text in this guide, because I like pretentious self-reference in writing."
"7","Code.md_7","Also because hopefully you're familiar enough with the text to get a feel for the techniques in practice."
"8","Code.md_8","There is a `data_preparation."
"9","Code.md_9","R` script, which gets the text from the guide and stores it in a usable format for the code."
"10","Code.md_10","The `data_preparation."
"11","Code.md_11","R` script is `source`d at the beginning of the `LSA.R` script, which uses the `clean_word_counts` object it creates."
"12","Code.md_12","The `data_preparation."
"13","Code.md_13","R` script is `source`d at the beginning of the `LDA.R` script, which uses the `full_text` and `full_word_counts` objects it creates."
"14","Code.md_14","You need the `sentences.csv` file which `data_preparation."
"15","Code.md_15","R` generates."
"16","Code.md_16","This is included in this repo, but you may want to regenerate it."
"17","Code.md_17","You also need to go to download some vectors of your choice (these files are large so it might take a while) and store them somewhere you can access."
"18","Code.md_18","The code works by you writing in the appropriate filepath to access the vectors."
"19","Code.md_19","Hopefully each piece of code is annotated so that you can run it."
"20","Code.md_20","If not, or if something is insufficiently clear, please let me know."
"21","Code.md_21","___ [Back to contents](.."
"22","Code.md_22","/README.md)"
"23","FeatureSelection.md_1","* Decisions about how to do these things are usually made by trial and error."
"24","FeatureSelection.md_2","This is usually easily accomplished - in R, for example, there are various different ways of doing it but it's always just one line of code, or a parameter set in a function."
"25","FeatureSelection.md_3","Despite the practical ease of achieving this, it still needs thought."
"26","FeatureSelection.md_4","You can choose to replace the punctuation marks with a space, or remove them altogether."
"27","FeatureSelection.md_5","On the other hand, removing punctuation altogether can lead to problems in cases where someone has hyphenated a word."
"28","FeatureSelection.md_6","For example, we might want *cross-examination* to become *crossexamination*, but not want *Northern Ireland-related* to become the nonsensical *Northern Irelandrelated*."
"29","FeatureSelection.md_7","In my experience you are better off replacing punctuation marks with spaces and then dealing with specific hyphenated words as special cases (but I could be wrong)."
"30","FeatureSelection.md_8","There are different methods to follow when deciding which words to remove from analysis."
"31","FeatureSelection.md_9","They are not mutually exclusive, and in fact there will often be some overlap between them."
"32","FeatureSelection.md_10","Care is needed when thinking about which of these methods to use, and which order to apply them in."
"33","FeatureSelection.md_11","Where we have used multiple methods we have often followed a 'belt and braces' approach and not worried about the redundancy in parts of our procedure."
"34","FeatureSelection.md_12","It would be better to be clear from the start about what we want to remove, because redundant processes at best waste time, and at worst create unforeseen consequences."
"35","FeatureSelection.md_13","Words often take slightly different forms that have grammatical meaning but don't change what the core concept is about."
"36","FeatureSelection.md_14","For example, *save*, *saved*, *saves*, and *saving* all encode the same idea but are used to indicate who is doing the action and when."
"37","FeatureSelection.md_15","We may not want this granularity of information in natural language processing projects."
"38","FeatureSelection.md_16","Perhaps we don't have enough data to reliably create a model in which these four words are represented as being similar but different, or perhaps we suspect our users will be able to deduce that subtlety for themselves and are more likely to be looking for the concept of saving than for the specific word *saving*."
"39","FeatureSelection.md_17","For example, consider the sentence > As Gregor Samsa awoke one morning from uneasy dreams he found himself transformed in his bed into a gigantic insect. >*awok*, *bed*, *dream*, *found*, *gigant*, *gregor*, *insect*, *morn*, *samsa*, *transform*, *uneasi*   (in alphabetical order)."
"40","FeatureSelection.md_18","___ [Back to contents](README.md)"
"41","Glossary.md_1","An approach to *embedding* in which the order of words in a *document* is not considered, just the presence or absence (or sometimes quantity) of terms."
"42","Glossary.md_2","* To a bag of words model, ""leave your stuff with the civil servants”, “the civil servants leave with your stuff"", and “stuff the civil servants, with your leave” are all seen as the same set {civil, leave, servants, stuff, the, with, your}, and thus all have the same meaning."
"43","Glossary.md_3","* An Artifical Intelligence using a bag of words model would not get any enjoyment from the Wendy Cope poem [""*The Uncertainty of the Poet*""](http://frombooksofpoems.blogspot.co.uk/2007/03/uncertainty-of-poet-by-wendy-cope.html)."
"44","Glossary.md_4","A catch-all term for a group of algorithms that aim to collect *documents* into clusters."
"45","Glossary.md_5","The idea is that the documents within each cluster have something in common, and in particular that they have more in common with each other than with documents from outside the cluster."
"46","Glossary.md_6","Clustering algorithms typically require some measure of distance (or, to some extent equivalently, similarity) between *documents* in a vector space."
"47","Glossary.md_7","There are a lot of different algorithms that can be used, all with pros and cons depending on the situation."
"48","Glossary.md_8","The set of text *document*s that you are analysing."
"49","Glossary.md_9","* The set of written parliamentary questions that the Ministry of Justice has answered since a give date."
"50","Glossary.md_10","* The set of sentences from all prison inspection reports since a given date."
"51","Glossary.md_11","* A set of emails sent to a particular person."
"52","Glossary.md_12","A way of measuring similarity between *documents* after they have been *embedded* as vectors."
"53","Glossary.md_13","The gist is that the similarity between any two documents *a* and *b* is judged by the angle _&theta;_. between their vectors **a** and **b**."
"54","Glossary.md_14","To be specific, we use the cosine of this angle: <a href=""https://www.codecogs.com/eqnedit.php?"
"55","Glossary.md_15","latex=\text{Similarity&space;of&space;}\textbf{a}\text{&space;and&space;}\textbf{b}&space;=&space;\cos&space;\theta"" target=""_blank""><img src=""https://latex.codecogs.com/gif.latex?"
"56","Glossary.md_16","\text{Similarity&space;of&space;}\textbf{a}\text{&space;and&space;}\textbf{b}&space;=&space;\cos&space;\theta"" title=""\text{Similarity of }\textbf{a}\text{ and }\textbf{b} = \cos \theta"" /></a> The rationale for this is that the vector space into which we *embed* our *documents* is defined such that the dimensions in it approximately relate to the concepts within the *documents*."
"57","Glossary.md_17","The vector for a *document* points in the directions of the concepts that *document* contains."
"58","Glossary.md_18","Therefore two *documents* with similar conceptual content will have vectors that point in similar directions: the angle between their vectors will be relatively small, so the cosine of this angle will be larger than that between documents with no conceptual similarity."
"59","Glossary.md_19","Note that cosine similarity is a [similarity measure](https://en.wikipedia.org/wiki/Similarity_measure) rather than a [metric](https://en.wikipedia.org/wiki/Metric_(mathematics))."
"60","Glossary.md_20","A text object, the collection of which make up your *corpus*."
"61","Glossary.md_21","If you are doing work on *Search* or *Topics*, the *document*s will be the objects which you will be finding similarities between in order to group them topically."
"62","Glossary.md_22","The length and definition of a *document* will depend on the question you are answering."
"63","Glossary.md_23","* A written parliamentary question."
"64","Glossary.md_24","* A text message sent to a mobile phone."
"65","Glossary.md_25","* A novel."
"66","Glossary.md_26","* A sentence from a prison report."
"67","Glossary.md_27","The process whereby *documents* or words are coded up as a vector in some (typically very high-dimensional) vector space."
"68","Glossary.md_28","* Counting the number of times a *document* contains the words biscuit or biscuits and assigning this number to the  *document*."
"69","Glossary.md_29","* Latent semantic analysis."
"70","Glossary.md_30","* Neural network models like Word2Vec, Doc2Vec, FastText, etc. <a align='center' href=""https://www.codecogs.com/eqnedit.php?"
"71","Glossary.md_31","latex=\log&space;\frac{N}{n_t}"" target=""_blank""><img src=""https://latex.codecogs.com/gif.latex?"
"72","Glossary.md_32","\log&space;\frac{N}{n_t}"" title=""\log \frac{N}{n_t}"" /></a> weighting for a word *t*, where *N* is the total number of *documents* in the *corpus*, and *n~t~* is the number of *documents* that contain *t*."
"73","Glossary.md_33","Transforming a vector so that it has unit length, by dividing the initial vector by its (Euclidean) length."
"74","Glossary.md_34","If you are using *cosine similarity* to measure similarities between *document* vectors, normalising the vectors is often a good idea because, for vectors **a** and **b** <a href=""https://www.codecogs.com/eqnedit.php?"
"75","Glossary.md_35","latex=\textbf{a}.\textbf{b}&space;=&space;|\textbf{a}||\textbf{b}|&space;\cos&space;\theta"" target=""_blank""><img src=""https://latex.codecogs.com/gif.latex?"
"76","Glossary.md_36","\textbf{a}.\textbf{b}&space;=&space;|\textbf{a}||\textbf{b}|&space;\cos&space;\theta"" title=""\textbf{a}.\textbf{b} = |\textbf{a}||\textbf{b}| \cos \theta"" /></a> where  *\theta* is the angle between **a** and **b**."
"77","Glossary.md_37","If we denote the normalised versions of **a** and **b** as **a'** and **b'** respectively, we have <a href=""https://www.codecogs.com/eqnedit.php?"
"78","Glossary.md_38","latex=\inline&space;|\textbf{a}|&space;=&space;|\textbf{b}|&space;=&space;1"" target=""_blank""><img src=""https://latex.codecogs.com/gif.latex?"
"79","Glossary.md_39","\inline&space;|\textbf{a}|&space;=&space;|\textbf{b}|&space;=&space;1"" title=""|\textbf{a}| = |\textbf{b}| = 1"" /></a>, so <a href=""https://www.codecogs.com/eqnedit.php?"
"80","Glossary.md_40","latex=\cos&space;\theta&space;=&space;\textbf{a}^{\prime}.\textbf{b}^{\prime}"" target=""_blank""><img src=""https://latex.codecogs.com/gif.latex?"
"81","Glossary.md_41","\cos&space;\theta&space;=&space;\textbf{a}^{\prime}.\textbf{b}^{\prime}"" title=""\cos \theta = \textbf{a}^{\prime}.\textbf{b}^{\prime}"" /></a>  Dot products are typically much quicker to compute than cosines, and normalisation is quick, so this saves time."
"82","Glossary.md_42","The practice of reducing words to their roots."
"83","Glossary.md_43","This reduces the number of words in a vocabulary, and focusses *embedding* on the concept that the word is trying to encode, rather than the grammatical context of the word."
"84","Glossary.md_44","We have generally done it using the [Porter algorithm](https://tartarus.org/martin/PorterStemmer/), which has implementations in a number of programming languages including R."
"85","Glossary.md_45","* Reducing plural nouns to the singular - *biscuits* to *biscuit*."
"86","Glossary.md_46","* Reducing verbs to their roots - *accused*, *accuse*, *accuses*, *accusing* all transformed to the root *accus*."
"87","Glossary.md_47","* You can see a list of words [here](https://tartarus.org/martin/PorterStemmer/voc.txt) and their stemmed equivalents [here](https://tartarus.org/martin/PorterStemmer/output.txt)."
"88","Glossary.md_48","Words routinely removed from *document*s at an early stage of the analysis."
"89","Glossary.md_49","* These can be taken from a standard list of English stopwords (e.g. words like *and*, *of*, or *the*)."
"90","Glossary.md_50","* They can also be from a bespoke project-specific list (e.g. the phrase *Secretary of State* in the case of Parliamentary Questions)."
"91","Glossary.md_51","A matrix, the columns of which are the vectors representing our *documents*, and the rows the words in our *vocabulary*."
"92","Glossary.md_52","Used in [*Latent Semantic Analysis*](LSA.md)."
"93","Glossary.md_53","Defines a subspace of our initial vector space, the rank of which is the smaller out of the number of documents and the size of the vocabulary."
"94","Glossary.md_54","Sometimes algorithms require a Document-Term Matrix (DTM) instead of a TDM; this is just the transpose of the TDM."
"95","Glossary.md_55","Beware that the terminology for these objects can be confused; for example, in R the package `lsa` contains the key function `lsa()` which will do the singular value decomposition that you want (see [LSA page for details](LSA.md))."
"96","Glossary.md_56","This function claims that its input must be > ...a document-term matrix ... containing *documents in colums, terms in rows*... (emphasis mine)."
"97","Glossary.md_57","However, the `TermDocumentMatrix()` function from the `tm` package, [along with Wikipedia](https://en.wikipedia.org/wiki/Document-term_matrix), agrees with our definition above."
"98","Glossary.md_58","The semantics aren't important, but care needs to be taken because when you do a singular value decomposition as part of LSA, you need to know which of the three matrices created corresponds to terms, and which to *documents*."
"99","Glossary.md_59","The set of all words used in the *corpus*, after *stopwords* have been removed and *stemming* has been done (where appropriate)."
"100","Glossary.md_60","* The *corpus* of lines taken from Dr Seuss' children's book ""*Green Eggs and Ham*"" has the following *vocabulary* ([source](https://wordobject.wordpress.com/2011/05/18/lists-green-eggs-and-ham/)): > a, am, and, anywhere, are, be, boat, box, car, could, dark, do, eat, eggs, fox, goat, good, green, ham, here, house, I, if, in, let, like, may, me, mouse, not, on, or, rain, Sam, say, see, so, thank, that, the, them, there, they, train, tree, try, will, with, would, you * The *vocabulary* of that  *corpus* after removing *stopwords* featured in the [SMART list](http://www.lextek.com/manuals/onix/stopwords2.html) (note that all stopwords in this list are lower case): > boat, box, car, dark, eat, eggs, fox, goat, good, green, ham, house, I, mouse, rain, Sam, train, tree."
"101","Glossary.md_61","* The *vocabulary* of that  *corpus* after making lower case, removing *stopwords* from the [SMART list](http://www.lextek.com/manuals/onix/stopwords2.html) and *stemming* (note that because we made everything lower case, the word ""*I*"" is now removed: > boat, box, car, dark, eat, egg, fox, goat, good, green, ham, hous, mous, rain, sam, train, tree."
"102","Glossary.md_62","The scheme by which we go from a vector of counts of each word in the *vocabulary* for a given document to an *embedding*."
"103","Glossary.md_63","Typically made up of three elements: term frequency, *inverse document frequency*, and *normalisation*."
"104","Glossary.md_64","* Simply count the occurrences of each word in the the vocabulary, and don't worry about *inverse document frequency* or *normalisation*."
"105","Glossary.md_65","* Use a Boolean approach to term frequency, recording only the appearance (score 1) or non-appearance (score 0) of words rather than their frequency; multiply this by the *inverse document frequency* score for the word; then *normalise* the resultant vector."
"106","Glossary.md_66","This is the weighting scheme used in the Parliamentary Analysis Tool."
"107","Glossary.md_67","___ [Back to contents](README.md)"
"108","Intro.md_1","It's the use of computers to analyse text."
"109","Intro.md_2","This is a broad area of endeavour, depending on what the question is that you're trying to answer."
"110","Intro.md_3","Below we list four things that people sometimes bracket under natural language processing."
"111","Intro.md_4","The solutions to problems under these four headings are sometimes related, but just as often are not."
"112","Intro.md_5","At present this repo summarises some findings we've made from creating products to answer questions relating to [*Search*](Search.md)  and [*Topics*](Topics.md)."
"113","Intro.md_6","You can click the headings to see our thoughts in these areas."
"114","Intro.md_7","If you are unsure about a piece of jargon or terminology you might find it represented in our [glossary](Glossary.md)."
"115","Intro.md_8","Finding parts of the text that are about a particular topic of interest (e.g. allow the user to search for parts of the text that are about biscuits)."
"116","Intro.md_9","Find key identifiers in the text (e.g. tag all occurrences of people's names, or of names of biscuits)."
"117","Intro.md_10","Understand some type of linguistic input and translate it to some action (e.g. a chatbot responding to typed input)."
"118","Intro.md_11","___ [Back to contents](README.md)"
"119","LDA.md_1","* Latent Dirichlet Allocation is a probabilistic method for [*Topic*](Topics.md) Modelling."
"120","LDA.md_2","* Unfortunately we have encountered two major problems with the results when we've tried this."
"121","LDA.md_3","These problems may be with our implementations rather than with the method, but we have tried on different data (and colleagues from another department independently had the same problems)."
"122","LDA.md_4","* We have stopped trying to implement LDA for the moment."
"123","LDA.md_5","* The set of distributions of words for each topic. !["
"124","LDA.md_6","Example SVD from rank 2 to rank 1](LDAresults.png) There are a few different methods for generating an LDA, and I'm not going to go through them in any kind of detail here."
"125","LDA.md_7","But I think the reader should be aware of what the algorithms all do in general."
"126","LDA.md_8","Typically an algorithm will do something like: start in a random place on the *(k (M + N) - k - M)*-dimensional surface we are interested in, and do a guided random walk that 'climbs the hill' towards increasing this likelihood function; stop once further improvement is sufficiently unlikely that we think we are on a (local) optimum."
"127","LDA.md_9","To increase the chances of finding the global optimum, we run the process several times, starting from different random locations each time."
"128","LDA.md_10","Unfortunately, we have not found it to quite work like that..."
"129","LDA.md_11","We have tried using LDA on PQs and on consultation responses and are yet to get it to work in a satisfying way."
"130","LDA.md_12","Additionally, colleagues at Department for Transport had been using this approach for some work they were doing on consultations until we warned them about Problem 1 below, which they realised on inspection that they also suffered from."
"131","LDA.md_13","We don't claim that LDA doesn't or can't work in general, just that we haven't yet managed to get it so that the results are usable."
"132","LDA.md_14","It may be that we just haven't found the correct set of parameters for the algorithm, or it may be that in these cases the data isn't appropriate for an LDA for reasons I don't fully comprehend."
"133","LDA.md_15","Perhaps we need to do [feature selection](FeatureSelection.md) first."
"134","LDA.md_16","Regardless of why, here are the two main problems with LDA results as we've found them."
"135","LDA.md_17","It should be added that since writing this I have written a toy bit of code on LDA for this repo (found in the `code` folder) and have not managed to replicate this problem."
"136","LDA.md_18","So maybe it was just the package that we and DfT were using, or maybe it's a problem that only crops up when you have a sufficiently large data set."
"137","LDA.md_19","The next problem is still present, however."
"138","LDA.md_20","This relates to problem 1, and in fact for a while this issue prevented us from discovering problem 1."
"139","LDA.md_21","All dimensionality reduction type approaches in natural language processing suffer from this to some extent, but the complexity of LDA makes it worse here in my opinion."
"140","LDA.md_22","Regardless of what we do, we're looking at a list of words that is somehow representative of this topic."
"141","LDA.md_23","But these words typically don't fit together in an easily-comprehensible way."
"142","LDA.md_24","It typically isn't the case that we get a list like: > Topic 1: banana, orange, grapefruit, peel, vitamin, five, watermelon More usual is that, whichever method we use, we get some slightly odd mix of words and we have to strain a bit to say what this Topic is 'about' semantically."
"143","LDA.md_25","___ [Back to contents](README.md)"
"144","LSA.md_1","* To choose *k* we have so far relied on good old trial and error."
"145","LSA.md_2","Although there are doubtless statistical measures for how ""good"" a value of *k* is (along the lines of plotting some loss of information and looking for an elbow in the plot), we have instead focussed on investigating what happens in the actual tool."
"146","LSA.md_3","Warning: this page includes both mathematical formulae and mathematical concepts."
"147","LSA.md_4","I've tried to keep it as simple as possible and to use examples where I can, but doubtless it can be improved upon. >boat, box, car, dark, eat, egg, fox, goat, good, green, ham, hous, mous, rain, sam, train, tree To continue our ""*Green Eggs and Ham*"" example, the line > I do not like green eggs and ham >{green, egg, ham} > Eat them!"
"148","LSA.md_5","Eat them!"
"149","LSA.md_6","Here they are!"
"150","LSA.md_7","becomes > {eat, eat} * just counting the frequency of terms; and * scoring 1 if a term is used and 0 if it isn't."
"151","LSA.md_8","This latter, ""Boolean"" scheme is what we use when doing LSA on Parliamentary Questions: the fact that a PQ mentions the word ""prison"" more than once (for example, asking for the same statistic about multiple named prisons) doesn't make it more about prisons."
"152","LSA.md_9","For example, the Ministry of Justice does not get many Parliamentary Questions about Britain leaving the EU."
"153","LSA.md_10","If we see the relatively rare term ""*Brexit*"" in a question it's a good guide to what the question is about."
"154","LSA.md_11","On the other hand, there are lots of questions about various aspects of prisons, and so the existence of the word ""*prison*"" in a question tells us less about its specific topical content. <a href=""https://www.codecogs.com/eqnedit.php?"
"155","LSA.md_12","latex=\inline&space;\frac{\mathbf{a}.\mathbf{b}}{|\mathbf{a}||\mathbf{b}|}&space;=&space;\cos&space;\theta"" target=""_blank""><img src=""https://latex.codecogs.com/gif.latex?"
"156","LSA.md_13","\inline&space;\frac{\mathbf{a}.\mathbf{b}}{|\mathbf{a}||\mathbf{b}|}&space;=&space;\cos&space;\theta"" title=""\frac{\mathbf{a}.\mathbf{b}}{|\mathbf{a}||\mathbf{b}|} = \cos \theta"" /></a> * is of rank *k*; and There are plenty of other sites that go through the maths behind Singular value decomposition (SVD) in detail so I won't do that here."
"157","LSA.md_14","Just to help you see the sort of thing, here's a toy example."
"158","LSA.md_15","It's necessarily of very low dimension so that it's possible for our tiny human brains to envision it. <a href=""https://www.codecogs.com/eqnedit.php?"
"159","LSA.md_16","latex=\inline&space;\mathbf{M}&space;=&space;\begin{bmatrix}&space;1&space;&&space;2&space;&&space;3&space;\\&space;2&space;&&space;4&space;&&space;1&space;\end{bmatrix}"" target=""_blank""><img src=""https://latex.codecogs.com/gif.latex?"
"160","LSA.md_17","\inline&space;\mathbf{M}&space;=&space;\begin{bmatrix}&space;1&space;&&space;2&space;&&space;3&space;\\&space;2&space;&&space;4&space;&&space;1&space;\end{bmatrix}"" title=""original embedding"" /></a> We then do a singular-value decomposition and get <a href=""https://www.codecogs.com/eqnedit.php?"
"161","LSA.md_18","latex=\mathbf{M}&space;=&space;\begin{bmatrix}&space;0.61&space;&&space;0.79&space;\\&space;0.79&space;&&space;-0.61&space;\end{bmatrix}&space;\begin{bmatrix}&space;5.56&space;&&space;0&space;\\&space;0&space;&&space;2.01&space;\end{bmatrix}&space;\begin{bmatrix}&space;0.39&space;&&space;0.79&space;&&space;0.47&space;\\&space;-0.21&space;&&space;-0.42&space;&&space;0.88&space;\end{bmatrix}"" target=""_blank""><img src=""https://latex.codecogs.com/gif.latex?"
"162","LSA.md_19","\mathbf{M}&space;=&space;\begin{bmatrix}&space;0.61&space;&&space;0.79&space;\\&space;0.79&space;&&space;-0.61&space;\end{bmatrix}&space;\begin{bmatrix}&space;5.56&space;&&space;0&space;\\&space;0&space;&&space;2.01&space;\end{bmatrix}&space;\begin{bmatrix}&space;0.39&space;&&space;0.79&space;&&space;0.47&space;\\&space;-0.21&space;&&space;-0.42&space;&&space;0.88&space;\end{bmatrix}"" title=""Singular value decomposition"" /></a> where all values are given to 2 decimal places, rather than giving precise expressions."
"163","LSA.md_20","Our best 1-dimensional approximation to this is then <a href=""https://www.codecogs.com/eqnedit.php?"
"164","LSA.md_21","latex=\begin{matrix}&space;\mathbf{M_1}&space;&&space;=&space;&&space;\begin{bmatrix}&space;0.61&space;&&space;0.79&space;\\&space;0.79&space;&&space;-0.61&space;\end{bmatrix}&space;\begin{bmatrix}&space;5.56&space;&&space;0&space;\\&space;0&space;&&space;0&space;\end{bmatrix}&space;\begin{bmatrix}&space;0.39&space;&&space;0.79&space;&&space;0.47&space;\\&space;-0.21&space;&&space;-0.42&space;&&space;0.88&space;\end{bmatrix}&space;\\&space;&&space;=&space;&&space;\begin{bmatrix}&space;1.33&space;&&space;2.67&space;&&space;1.59&space;\\&space;1.74&space;&&space;3.48&space;&&space;2.08&space;\end{bmatrix}&space;\end{matrix}"" target=""_blank""><img src=""https://latex.codecogs.com/gif.latex?"
"165","LSA.md_22","\begin{matrix}&space;\mathbf{M_1}&space;&&space;=&space;&&space;\begin{bmatrix}&space;0.61&space;&&space;0.79&space;\\&space;0.79&space;&&space;-0.61&space;\end{bmatrix}&space;\begin{bmatrix}&space;5.56&space;&&space;0&space;\\&space;0&space;&&space;0&space;\end{bmatrix}&space;\begin{bmatrix}&space;0.39&space;&&space;0.79&space;&&space;0.47&space;\\&space;-0.21&space;&&space;-0.42&space;&&space;0.88&space;\end{bmatrix}&space;\\&space;&&space;=&space;&&space;\begin{bmatrix}&space;1.33&space;&&space;2.67&space;&&space;1.59&space;\\&space;1.74&space;&&space;3.48&space;&&space;2.08&space;\end{bmatrix}&space;\end{matrix}"" title=""Rank 1 approximation to our initial embedding"" /></a> !["
"166","LSA.md_23","Example SVD from rank 2 to rank 1](2dSVD2.png) <a href=""https://www.codecogs.com/eqnedit.php?"
"167","LSA.md_24","latex=\begin{bmatrix}&space;5.56&space;&&space;0&space;\\&space;0&space;&&space;0&space;\end{bmatrix}&space;\begin{bmatrix}&space;0.39&space;&&space;0.79&space;&&space;0.47&space;\\&space;-0.21&space;&&space;-0.42&space;&&space;0.88&space;\end{bmatrix}&space;=&space;\begin{bmatrix}&space;2.19&space;&&space;4.39&space;&&space;2.62&space;\\&space;0&space;&&space;0&space;&&space;0&space;\end{bmatrix}"" target=""_blank""><img src=""https://latex.codecogs.com/gif.latex?"
"168","LSA.md_25","\begin{bmatrix}&space;5.56&space;&&space;0&space;\\&space;0&space;&&space;0&space;\end{bmatrix}&space;\begin{bmatrix}&space;0.39&space;&&space;0.79&space;&&space;0.47&space;\\&space;-0.21&space;&&space;-0.42&space;&&space;0.88&space;\end{bmatrix}&space;=&space;\begin{bmatrix}&space;2.19&space;&&space;4.39&space;&&space;2.62&space;\\&space;0&space;&&space;0&space;&&space;0&space;\end{bmatrix}"" title=""Coordinates of black points in basis of new embedding"" /></a> <a href=""https://www.codecogs.com/eqnedit.php?"
"169","LSA.md_26","latex=\begin{bmatrix}&space;0.61&space;&&space;0.79&space;\\&space;0.79&space;&&space;-0.61&space;\end{bmatrix}&space;\begin{bmatrix}&space;5.56&space;&&space;0&space;\\&space;0&space;&&space;0&space;\end{bmatrix}&space;=&space;\begin{bmatrix}&space;3.38&space;&&space;0&space;\\&space;4.41&space;&&space;0&space;\end{bmatrix}"" target=""_blank""><img src=""https://latex.codecogs.com/gif.latex?"
"170","LSA.md_27","\begin{bmatrix}&space;0.61&space;&&space;0.79&space;\\&space;0.79&space;&&space;-0.61&space;\end{bmatrix}&space;\begin{bmatrix}&space;5.56&space;&&space;0&space;\\&space;0&space;&&space;0&space;\end{bmatrix}&space;=&space;\begin{bmatrix}&space;3.38&space;&&space;0&space;\\&space;4.41&space;&&space;0&space;\end{bmatrix}"" title=""location of new embedding subspace"" /></a> Additionally, this allows us to derive an equation describing the line of which the black points in the diagram sit - it is *y* = (4.41 / 3.38) *x*."
"171","LSA.md_28","Finally, note that in terms of being an algorithm that * uses matrix algebra; and * results in the creation of a reduced-rank subspace; which * is somehow closest to the original space; and * has a basis that is a linear combination of the original basis vectors SVD is clearly similar in ethos to principal component analysis."
"172","LSA.md_29","[Obviously the two can be formally compared](https://intoli.com/blog/pca-and-svd/)."
"173","LSA.md_30","Deep breath now, the hardcore (well, first year Linear Algebra course, which is hard for me) maths is done."
"174","LSA.md_31","We can now see the point of all this."
"175","LSA.md_32","First, a recap: We might say that, once we take randomness out of the picture, the points that end up closer to one another are ""truly"" closer in information content than ones that end up far away."
"176","LSA.md_33","Reducing the dimensionality gives us a model that doesn't overfit - in particular, it doesn't take synonyms as being separate entities, but (at least hopefully) sees them as meaning similar things (because synonyms will tend to co-occur with similar terms)."
"177","LSA.md_34","Choosing the value of *k* is not necessarily simple."
"178","LSA.md_35","If you pick *k* too large then you risk paying too much attention to outliers like rare words; this is conceptually similar to overfitting."
"179","LSA.md_36","If you pick *k* too small then you risk throwing away too much information."
"180","LSA.md_37","Consequently we have so far used trial and error to determine *k* and not worried too much about needing to be incredibly precise."
"181","LSA.md_38","The important thing is to find a value of *k* that gives the end user something that seems meaningful and useful."
"182","LSA.md_39","Writing this has allowed me to think about our process in more detail, and I am aware that there are potentially some issues with it."
"183","LSA.md_40","In practice it works fine in the Parliamentary Analysis Tool, so I'm not too worried."
"184","LSA.md_41","But if anyone knows the answers to the following questions we might be able to do better (or at least set things on a firmer mathematical grounding)."
"185","LSA.md_42","The process we follow goes As I say, in practice it seems like the results are still fine, but if anyone knows how to produce a low-rank approximation that minimises changes to angles between points, and can implement it in R, I would be a lot happier."
"186","LSA.md_43","It might be equivalent (or perhaps: better) to be able to find the optimal *k*-dimensional hyperspherical approximation to data which sits on an *r*-dimensional hypersphere."
"187","LSA.md_44","[These people have done this, but I haven't looked at implementing it in R](http://citeseerx.ist.psu.edu/viewdoc/download?"
"188","LSA.md_45","doi=10.1.1.70.5407&rep=rep1&type=pdf)."
"189","LSA.md_46","Their first definition of 'optimal', the 'Fidelity test' seems like it would be appropriate in our case."
"190","LSA.md_47","___ [Back to contents](README.md)"
"191","NNmodels.md_1","* There are a wide variety of such methods; for example Word2Vec is actually not one but two separate methods (CBOW and skip-gram)."
"192","NNmodels.md_2","* The output vectors encode both semantic and syntactic meaning, and we can do some algebra with them."
"193","NNmodels.md_3","For example with the Word2Vec pretrained vectors we get **France** + **Berlin** - **Germany** = **Paris**, and also **faster** + **warm** - **fast** = **warmer** (where **term** is the vector representing ""*term*"", additions and subtractions are as usual with vectors, and the equals operator in this case means ""closest pre-defined vector"", e.g."
"194","NNmodels.md_4","**Paris** is the closest predefined vector to the vector we get when doing the sum **France** + **Berlin** - **Germany**)."
"195","NNmodels.md_5","* We need to do some testing on our results to make sure that they are good."
"196","NNmodels.md_6","This is a mix of automated tests and simply eyeballing the output to make sure it looks ok."
"197","NNmodels.md_7","We can of course add and subtract vectors in the usual way, so all that is needed to make sense of the results is to define the equals operator such that **A** + **B** = **C** is understood to mean With this definition, and using the standard downloadable Word2Vec vectors pre-trained on Google News we get that, for example, **France** + **Berlin** - **Germany** = **Paris** and **faster** + **warm** - **fast** = **warmer**  Not that this sort of arithmetic is particularly directly useful when doing [search](Search.md) or findings [topics](Topics.md), of course."
"198","NNmodels.md_8","But perhaps it provides some credibility to the model."
"199","NNmodels.md_9","At worst it's a neat trick."
"200","NNmodels.md_10","The alternative to using pre-trained vectors is of course training your own."
"201","NNmodels.md_11","In practice we have done this using Python and the excellent `gensim` library."
"202","NNmodels.md_12","You should of course do some testing to help improve your chosen mix of parameters and meta-parameters."
"203","NNmodels.md_13","There are actually two Word2Vec models: CBOW (Continuous Bag-of-Words) and Skip-gram."
"204","NNmodels.md_14","I won't go into too much detail about how they work, because you can find plenty of description online (e.g. [here](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/) and [here](https://medium.com/scaleabout/a-gentle-introduction-to-doc2vec-db3e8c0cce5e))."
"205","NNmodels.md_15","But I think it's important to give a brief overview. > we can have lots of good fun that is funny (taken from The Cat in the Hat, by Dr Seuss)."
"206","NNmodels.md_16","If we focus on the word ""*good*"", we can see that it is preceded by the four words *can*, *have*, *lots*, *of*, and followed by the four words *fun*, *that*, *is*, *funny*."
"207","NNmodels.md_17","This idea of words existing in the centre of a sliding window of the words around them is central to Word2Vec."
"208","NNmodels.md_18","In this example I have chosen a window size of 4 words, but in practice we can set this to whatever we like."
"209","NNmodels.md_19","The major distinction between CBOW and skip-gram models is in the task that the neural network is given."
"210","NNmodels.md_20","In the case of CBOW, the task is to predict the focal word (*good*) from the context ({*can*, *have*, *lots*, *of*; *fun*, *that*, *is*, *funny*})."
"211","NNmodels.md_21","Conversely, for skip-gram, the task is to predict the context words ({*can*, *have*, *lots*, *of*; *fun*, *that*, *is*, *funny*}) from the focal word *good*."
"212","NNmodels.md_22","In practice we've found the two methods to produce broadly similar results."
"213","NNmodels.md_23","According to one of the inventors of Word2Vec, Tomas Mikolov, > Skip-gram: works well with small amount of the training data, represents well even rare words or phrases."
"214","NNmodels.md_24","CBOW: several times faster to train than the skip-gram, slightly better accuracy for the frequent words In practice, Mikolov (yes, him again) claims that a combination of both algorithms should be used, but elsewhere I've seen it claimed that DM should be superior."
"215","NNmodels.md_25","For both Doc2Vec methods you can also get the model to output word vectors (at least in gensim)."
"216","NNmodels.md_26","Whereas Word2Vec and Doc2Vec were created by Google, fastText is the brainchild of Facebook."
"217","NNmodels.md_27","Oh, also of that Mikolov chap again, after he (presumably) got poached."
"218","NNmodels.md_28","As far as I know (which isn't much) fastText is an extension of Word2Vec in which words are broken down into their constituent parts, and these parts are used as input in addition to the words."
"219","NNmodels.md_29","So for example, even if we didn't know what ""psychopharmacotherapy"" meant we might be able to guess from the bits of words that make it up: ""psycho"", ""pharmaco"", ""therapy""."
"220","NNmodels.md_30","The beauty of having a well-defined set of tests is that we are able to use them to compare between models, and thus navigate our way better through the forest of different options (CBOW, skip-gram, DBOW, DM, plus parameter choices for all of them)."
"221","NNmodels.md_31","We can search the parameter (and meta-parameter) space, compute the test results, and use the model that gives us the best answers."
"222","NNmodels.md_32","> The economy, nonetheless, has yet to exhibit sustainable growth.  and  > But the economy hasn't shown signs of sustainable growth."
"223","NNmodels.md_33","Typically this is how we determine between models."
"224","NNmodels.md_34","The above two tests are more important because they capture some element of the actual use case of our model."
"225","NNmodels.md_35","However, it is also interesting to see how our model does when comparing meanings of actual words."
"226","NNmodels.md_36","We do this by doing some word arithmetic like that seen above."
"227","NNmodels.md_37","We've got a big set of quartets of words of the form > {work, works, walk, walks} or > {thinking, thought, writing, wrote} or various other combinations of semantic and syntactic variation."
"228","NNmodels.md_38","We go through this massive list, find examples where we have all four words in the list, and then do a calculation like **word_2** - **word_1** + **word_3** = **V** We then see if **word_4** is one of the closest vectors in our set to the vector **V**, ""closest"" here meaning something like ""in the top 10""."
"229","NNmodels.md_39","Obviously this sort of testing is harder to do systematically, but in the end it's the most important thing because it best reflects the purpose of all of this work."
"230","NNmodels.md_40","___ [Back to contents](README.md)"
"231","README.md_1","This repo contains thoughts and guidance about the use of Natural Language Processing, based on the experience of using these techniques in a few projects."
"232","README.md_2","* [Search through a set of documents](Search.md) * [Find topics in a set of documents](Topics.md) * [Feature Selection](FeatureSelection.md) * [Latent Semantic Analysis (LSA)](LSA.md) * [Latent Dirichlet Allocation (LDA)](LDA.md) * [Word2Vec, Doc2Vec, fastText (Neural Network models)](NNmodels.md) * There is code so you can try out these techniques in practice, located in the `code` folder * [More details about it here](code/Code.md)"
"233","Search.md_1","* Following [feature selection](FeatureSelection.md), some methodologies we've had success with are [Latent Semantic Analysis](LSA.md) and [Neural Network models like Word2Vec and Doc2Vec](NNmodels.md)."
"234","Search.md_2","* All methodologies (that we've tried) are subject to improvements using a lot of sleight-of-hand and parameter tweaking (some of this is part of [feature selection](FeatureSelection.md) and some is tweaking model parameters)."
"235","Search.md_3","Since language is hard, we dodge the problem by instead translating it into something that's easy: geometry (Editor's note: I am being glib - in fact bitter personal experience tells me that linear algebra is not in fact easy)."
"236","Search.md_4","For centuries Clever People have established rigorous foundations and proven theorems in the realm of linear algebra, meaning that well-defined statements can be made about such fantastical things as geometries in spaces with thousands of dimensions."
"237","Search.md_5","In particular, we can easily define [rigorous ways of measuring distances between points](https://en.wikipedia.org/wiki/Metric_space)."
"238","Search.md_6","As you may have noticed, the ""solution"" offered above simply begs the question, since we have simply moved from to Some models that we've used are [Latent Semantic Analysis](LSA.md), and [Neural Network models like Word2Vec and Doc2Vec](NNmodels.md) (click to read about them)."
"239","Search.md_7","In all of our use cases, a surprising amount of value can suddenly be added to the model following small changes to [feature selection](FeatureSelection.md) or to model parameters."
"240","Search.md_8","This is not reassuring from a Quality Assurance perspective."
"241","Search.md_9","Which of these things is most appropriate depends upon what the users want."
"242","Search.md_10","___ [Back to contents](README.md)"
"243","TLDR.md_1","* Following [feature selection](FeatureSelection.md), some methodologies we've had success with are [Latent Semantic Analysis](LSA.md) and [Neural Network models like Word2Vec and Doc2Vec](NNmodels.md)."
"244","TLDR.md_2","* All methodologies (that we've tried) are subject to improvements using a lot of sleight-of-hand and parameter tweaking (some of this is part of [feature selection](FeatureSelection.md) and some is tweaking model parameters)."
"245","TLDR.md_3","* There are various methods by which these things can be achieved."
"246","TLDR.md_4","* Latent Dirichlet Allocation is often mentioned in connection with this; we've never got it to work."
"247","TLDR.md_5","* For the unsupervised machine learning techniques, typically you have to define some parameter such as the number of topics, or the density of points in your vector space to define a cluster."
"248","TLDR.md_6","* Where topics are derived via an unsupervised machine learning methods their meanings can be hard to define in an appropriate, semantically-meaningful, human-understandable way."
"249","TLDR.md_7","* To choose *k* we have so far relied on good old trial and error."
"250","TLDR.md_8","Although there are doubtless statistical measures for how ""good"" a value of *k* is (along the lines of plotting some loss of information and looking for an elbow in the plot), we have instead focussed on investigating what happens in the actual tool."
"251","TLDR.md_9","* Latent Dirichlet Allocation is a probabilistic method for [*Topic*](Topics.md) Modelling."
"252","TLDR.md_10","* Unfortunately we have encountered two major problems with the results when we've tried this."
"253","TLDR.md_11","These problems may be with our implementations rather than with the method, but we have tried on different data (and colleagues from another department independently had the same problems)."
"254","TLDR.md_12","* We have stopped trying to implement LDA for the moment."
"255","Topics.md_1","* There are various methods by which these things can be achieved."
"256","Topics.md_2","* Latent Dirichlet Allocation is often mentioned in connection with this; we've never got it to work."
"257","Topics.md_3","* Where topics are derived via an unsupervised machine learning methods their meanings can be hard to define in an appropriate, semantically-meaningful, human-understandable way. * automatically discover what these underlying topics are; and then This question can actually be somewhat more general than just determining the topical content of the text."
"258","Topics.md_4","For example, if we take emails to have only one of two topics, 'spam' or 'ham', then creating a spam filter is an example of a problem in this area."
"259","Topics.md_5","[There is a full page on LDA, and our problems trying to implement it, here.]("
"260","Topics.md_6","LDA.md) Suffice it to say that we haven't got it to work satisfactorily."
"261","Topics.md_7","Using an unsupervised machine learning technique for obtaining topics means that there is inevitably an element of black box to proceedings."
"262","Topics.md_8","This air of mystery can have the unwanted side effect that for some topics it can be * hard to see why the machine has grouped them together, or * easy to see that the machine has grouped them together because of something we think of as trivial."
"263","Topics.md_9","___ [Back to contents](README.md)"
