{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook looks at a simple way to use pre-trained Neural Network embeddings using the PyMagnitude package\n",
    "\n",
    "# BEFORE YOU START\n",
    "# You need to have gone to this URL : https://github.com/plasticityai/magnitude#pre-converted-magnitude-formats-of-popular-embeddings-models\n",
    "# downloaded some vectors of you choice (these files can be massive so it might take a while), and stored\n",
    "# them somewhere you can access (e.g. with the file path I specify below).\n",
    "\n",
    "# The results below are using the fastText 'Medium' set of 1M words, with 300-dimensional embeddings,\n",
    "# trained on English wikipedia\n",
    "\n",
    "# YOU MAY ALSO\n",
    "# need to have run the data_preparation.R script if you lack the `sentences.csv` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymagnitude import *\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from nltk.corpus import stopwords\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So we can read wider text columns and not so much gets cut off\n",
    "pd.set_option('max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 787 ms, sys: 21.8 ms, total: 809 ms\n",
      "Wall time: 810 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#here's where you get your vectors in - you need to replace the path below with your own one\n",
    "vectors = Magnitude(\"/here/you/put/path/to/magnitude_file.magnitude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some things we can do with magnitude vectors for words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8671641"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I think this is Euclidean distance\n",
    "vectors.distance(\"biscuit\",\"macaroon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8671641, 1.0519347]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.distance(\"biscuit\",[\"macaroon\", \"cabbage\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.62401325"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is cosine similarity\n",
    "vectors.similarity(\"biscuit\",\"macaroon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.62401325, 0.4467167]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.similarity(\"biscuit\",[\"macaroon\", \"cabbage\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'macaroon'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using cosine similarity\n",
    "vectors.most_similar_to_given(\"biscuit\", [\"macaroon\", \"cabbage\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cabbage'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# odd one out\n",
    "vectors.doesnt_match([\"biscuit\", \"macaroon\", \"cabbage\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('biscuits', 0.8173666000366211),\n",
       " ('biscuity', 0.6973364353179932),\n",
       " ('cake', 0.6754744052886963),\n",
       " ('chocolate', 0.6582432985305786),\n",
       " ('oatcake', 0.6574128866195679),\n",
       " ('teacake', 0.6451709866523743),\n",
       " ('loaf', 0.6432216167449951),\n",
       " ('cornflake', 0.6331499814987183),\n",
       " ('flapjack', 0.6270235776901245),\n",
       " ('shortbread', 0.6263179779052734)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top n most similar\n",
    "vectors.most_similar(\"biscuit\", topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Berlin', 0.7935939431190491),\n",
       " ('Munich', 0.7534011006355286),\n",
       " ('Frankfurt', 0.7376378774642944),\n",
       " ('Cologne', 0.7260650992393494),\n",
       " ('Stuttgart', 0.7239525318145752),\n",
       " ('Leipzig', 0.7191416025161743),\n",
       " ('Vienna', 0.7057973146438599),\n",
       " ('Hamburg', 0.7021979093551636),\n",
       " ('Frankfurt-am-Main', 0.6996399164199829),\n",
       " ('DÃ¼sseldorf', 0.6979357004165649)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Paris + Germany - France = Berlin\n",
    "vectors.most_similar(positive=['Paris', 'Germany'], negative=['France'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Iraq', 0.7341831922531128),\n",
       " ('Kuwait', 0.6448562145233154),\n",
       " ('Mosul', 0.6440438628196716),\n",
       " ('Basra', 0.6172256469726562),\n",
       " ('Iraq--and', 0.6111494302749634),\n",
       " ('Al-Anbar', 0.6111178398132324),\n",
       " ('Baghdady', 0.6097682118415833),\n",
       " ('Al-Basrah', 0.6095267534255981),\n",
       " ('Al-Najaf', 0.6082150936126709),\n",
       " ('Basrah', 0.6069812774658203)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# England + Baghdad - London = Iraq\n",
    "vectors.most_similar(positive=['England', 'Baghdad'], negative=['London'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('East-Germany', 0.684023916721344),\n",
       " ('Berlin', 0.6813000440597534),\n",
       " ('Bonn', 0.6783359050750732),\n",
       " ('West-Germany', 0.6640989780426025),\n",
       " ('Germany-', 0.6633050441741943),\n",
       " ('Stuttgart', 0.6480668187141418),\n",
       " ('Dresden', 0.6451014280319214),\n",
       " ('Munich', 0.643798828125),\n",
       " ('Potsdam-Babelsberg', 0.6396905183792114),\n",
       " ('Freiburg', 0.6354193687438965)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It doesn't always work: Canberra + Germany - Australia = East Germany\n",
    "vectors.most_similar(positive=['Canberra', 'Germany'], negative=['Australia'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This embeds a search phrase or report sentence in our 300-dimensional vector space\n",
    "# by simply averaging over the words in the phrase\n",
    "# I have no doubt there are better ways of doing this, e.g. incorporating tf-idf weightings\n",
    "def embed_phrase(phrase):\n",
    "    try:\n",
    "        input_nopunc = re.sub(r'[^\\w\\s]','',phrase) #take out punctuation\n",
    "        input_lower = input_nopunc.lower().split() #make lower case and split by word\n",
    "        #now take out words not in word2vec model, and also words in nltk stopword list\n",
    "        clean_search = [word for word in input_lower if (word in vectors and word not in stopwords.words(\"English\"))]\n",
    "        if clean_search: #this means if clean_search isn't empty\n",
    "            unnorm_vector = np.mean(vectors.query(clean_search), axis = 0) #take mean of vectors of words that remain\n",
    "        else:\n",
    "            unnorm_vector = np.zeros(300) #set to zero if no words remain\n",
    "    except(TypeError):\n",
    "        unnorm_vector = np.zeros(300) #set to zero if it doesn't seem to be a string\n",
    "    #we normalize the result to length 1 so we can use dot products for cosine similarity\n",
    "    norm_array = preprocessing.normalize(unnorm_vector.reshape(-1,1), norm = 'l2', axis = 0)\n",
    "    return(np.concatenate(norm_array))\n",
    "\n",
    "# this allows you to search a phrase and compare it to a set of comparison sentences\n",
    "# again, no doubt this could be greatly improved.\n",
    "def search_phrase(phrase, comparison_set):\n",
    "    results_df = comparison_set\n",
    "    #we take our search phrase and compute its dot product with all of our guide sentences\n",
    "    #then we reorder by how similar the phrase is (larger dot product = more similar)\n",
    "    #and give the top 10\n",
    "    embedding = np.array(results_df['vectors'].values.tolist()).T\n",
    "    results_df['search_results'] = np.dot(embed_phrase(phrase), embedding)\n",
    "    return(results_df.sort_values(by=['search_results'])[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.28026947e-03, -2.43652239e-02,  1.32176944e-03, -9.69787687e-03,\n",
       "       -3.74229029e-02, -8.92745480e-02, -2.56897439e-03, -1.03370681e-01,\n",
       "       -6.57415316e-02,  1.29279522e-02,  8.24017625e-04,  7.51992390e-02,\n",
       "        2.91189346e-02, -2.35687736e-02,  5.70440330e-02,  4.56786007e-02,\n",
       "        1.60301358e-01,  1.96943642e-03,  1.33410349e-01,  3.72173935e-02,\n",
       "        2.48401333e-03,  7.24656135e-02,  6.67128386e-03,  4.35236953e-02,\n",
       "        3.62036712e-02, -1.57978833e-02,  5.85241467e-02, -2.75252834e-02,\n",
       "        4.37867455e-02, -3.61571088e-02,  1.13285063e-02, -2.19578911e-02,\n",
       "       -1.92831922e-02,  7.66812358e-03,  3.62293907e-02, -5.31622693e-02,\n",
       "        9.04731266e-03, -8.52390379e-03,  1.10962684e-03,  4.68735360e-02,\n",
       "       -2.17354670e-02, -1.00070961e-01, -9.81302857e-02, -9.39529669e-03,\n",
       "        4.40335227e-03, -2.58727856e-02,  1.92488786e-02, -5.05256131e-02,\n",
       "       -1.49174258e-02,  3.19914962e-03, -1.35877803e-02,  1.67603744e-03,\n",
       "        2.21653134e-02, -5.13803810e-02, -7.54389465e-02, -3.97904776e-02,\n",
       "        1.56661253e-02, -6.39626160e-02, -1.44768968e-01, -8.69245231e-02,\n",
       "       -4.93399501e-02,  8.51072930e-03,  2.11666003e-01, -1.15942806e-02,\n",
       "        2.94910986e-02,  5.58069870e-02, -6.09389693e-03,  6.76216409e-02,\n",
       "       -4.64824662e-02, -8.43283311e-02,  2.86841057e-02, -2.87931319e-02,\n",
       "        6.60953447e-02,  1.33042841e-03, -1.73678454e-02, -2.09496748e-02,\n",
       "       -3.32476236e-02, -6.30467385e-02, -3.38993967e-02,  1.26243476e-03,\n",
       "       -3.29608545e-02, -1.71698648e-02,  3.81438434e-03,  1.32885426e-01,\n",
       "        4.13149921e-03, -1.16397161e-02,  2.13067164e-03, -9.64173174e-04,\n",
       "       -3.78217995e-02, -5.15193539e-03, -1.27339931e-02,  3.69886868e-02,\n",
       "       -1.67766064e-01, -4.71423827e-02,  1.08763136e-01,  6.94216117e-02,\n",
       "       -1.92770828e-02,  6.03497438e-02,  4.98811640e-02, -1.28916102e-02,\n",
       "       -5.31006716e-02,  3.61709826e-04, -3.99939390e-03,  1.44440448e-04,\n",
       "       -4.62693162e-02, -2.20617995e-01,  8.78921431e-03,  4.21808437e-02,\n",
       "        3.15021463e-02,  6.52092919e-02, -5.82377575e-02,  4.46241423e-02,\n",
       "        7.93585926e-02,  5.54850176e-02,  6.37287647e-03,  2.38732081e-02,\n",
       "       -4.28756773e-02, -2.68710982e-02, -7.00625926e-02, -3.43196653e-02,\n",
       "       -1.28169870e-02,  1.85127445e-02, -3.57695706e-02, -1.83183029e-01,\n",
       "        9.78112817e-02,  1.13108335e-03,  1.03419209e-02, -9.19226855e-02,\n",
       "       -5.00338003e-02,  5.76336123e-02, -2.05343366e-02,  6.49174824e-02,\n",
       "       -3.22946906e-03, -9.54657141e-03, -7.79695883e-02, -7.16035441e-02,\n",
       "       -2.48756614e-02,  9.27762464e-02, -1.30076418e-02,  2.20725909e-02,\n",
       "       -5.79383830e-03, -3.87883745e-02, -6.07131096e-03, -2.41142195e-02,\n",
       "        4.79082577e-02,  6.71027154e-02,  2.08626837e-02,  1.30466782e-02,\n",
       "       -2.22602654e-02,  9.72489268e-03,  1.69801060e-02,  9.70375910e-02,\n",
       "        3.32811587e-02, -6.63768947e-02,  6.26092544e-03, -8.06729794e-02,\n",
       "       -7.13220378e-03,  3.24521549e-02, -2.39890907e-02,  3.40269343e-03,\n",
       "        2.01487038e-02, -3.38619575e-02, -2.90569151e-03,  2.04770565e-02,\n",
       "       -2.32404210e-02, -1.70471426e-02, -2.25862432e-02, -1.72944423e-02,\n",
       "        7.62891071e-03,  5.39487824e-02, -1.49774575e-03,  2.46082935e-02,\n",
       "       -2.06573159e-02, -4.53929082e-02,  2.88893580e-02,  1.20459981e-01,\n",
       "       -2.47964319e-02, -1.84397735e-02,  4.23969061e-04, -3.80148888e-02,\n",
       "        1.08625740e-01, -3.47708799e-02,  1.17191614e-03, -1.59638256e-01,\n",
       "        4.95294444e-02, -3.73813510e-02,  1.52369728e-02,  2.26802945e-01,\n",
       "       -3.20027918e-02,  1.16243083e-02,  4.11033034e-02,  1.59276262e-01,\n",
       "       -5.26110567e-02,  7.38882518e-04, -4.41474989e-02,  9.05307010e-02,\n",
       "        1.91506818e-02, -7.96159059e-02, -7.12293480e-03,  5.96217513e-02,\n",
       "       -1.63366199e-01,  1.04357980e-01,  7.10449517e-02,  6.39446005e-02,\n",
       "       -2.70788986e-02, -3.44649479e-02,  6.69798348e-03, -3.22976112e-02,\n",
       "        2.73473375e-02, -6.83618486e-02,  1.26708031e-01, -2.41828486e-02,\n",
       "       -2.18148120e-02, -3.37664001e-02,  7.89808482e-02,  2.38013342e-02,\n",
       "       -2.09949333e-02,  3.23087946e-02, -6.13101274e-02, -6.37964830e-02,\n",
       "        8.09582248e-02, -1.69871119e-03,  1.21889254e-02,  1.32810473e-01,\n",
       "        5.57901971e-02, -7.57667189e-03,  4.91595715e-02,  1.21370472e-01,\n",
       "        5.08492719e-03, -2.11056992e-02, -6.61255838e-03, -1.05227031e-01,\n",
       "       -1.08702555e-01,  9.79841687e-03,  4.95860241e-02,  4.61855717e-02,\n",
       "       -2.53480747e-02, -3.37899998e-02,  1.55376112e-02,  8.48953053e-03,\n",
       "        1.17619656e-01, -4.75333408e-02, -9.34155099e-03, -3.92557532e-02,\n",
       "       -9.75808054e-02,  4.80091684e-02,  5.93564333e-03, -5.14188828e-03,\n",
       "       -4.01025526e-02, -2.62064636e-02, -2.67107598e-02, -6.65480420e-02,\n",
       "        3.29647632e-03,  3.16362493e-02,  1.04835257e-03,  2.84470301e-02,\n",
       "       -4.33929525e-02,  1.20537532e-02,  1.95015315e-02, -1.81908754e-03,\n",
       "        1.67649221e-02,  5.30064739e-02,  3.38253565e-02, -2.40331963e-02,\n",
       "        1.09972861e-02,  6.03563478e-03, -6.19689710e-02, -1.84885953e-02,\n",
       "        3.99849564e-02, -1.81474432e-03,  4.29078862e-02,  1.45504668e-01,\n",
       "       -1.33573025e-01, -4.40021344e-02,  8.69530812e-02, -7.70415813e-02,\n",
       "        4.54186201e-02, -3.87175381e-02, -5.08772507e-02, -2.71983463e-02,\n",
       "        1.43552702e-02,  3.82375158e-02, -1.50599573e-02,  5.37734106e-03,\n",
       "       -2.43284758e-02,  2.10986882e-02, -2.35810224e-02, -4.17609029e-02,\n",
       "       -2.41251942e-02,  4.49815914e-02, -4.18738686e-02, -1.94134712e-02,\n",
       "       -5.25510907e-02, -1.98369175e-02,  1.62952021e-02, -2.05307137e-02,\n",
       "        2.14534793e-02,  4.20493484e-02,  4.64555435e-03, -2.45881942e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_phrase(\"jaffa cakes are my favourite and I like them the best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>FeatureSelection.md_1</td>\n",
       "      <td>* Decisions about how to do these things are usually made by trial and error.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>FeatureSelection.md_2</td>\n",
       "      <td>This is usually easily accomplished - in R, for example, there are various different ways of doing it but it's always just one line of code, or a parameter set in a function.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>FeatureSelection.md_3</td>\n",
       "      <td>Despite the practical ease of achieving this, it still needs thought.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>FeatureSelection.md_4</td>\n",
       "      <td>You can choose to replace the punctuation marks with a space, or remove them altogether.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>FeatureSelection.md_5</td>\n",
       "      <td>On the other hand, removing punctuation altogether can lead to problems in cases where someone has hyphenated a word.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                     ID  \\\n",
       "0           1  FeatureSelection.md_1   \n",
       "1           2  FeatureSelection.md_2   \n",
       "2           3  FeatureSelection.md_3   \n",
       "3           4  FeatureSelection.md_4   \n",
       "4           5  FeatureSelection.md_5   \n",
       "\n",
       "                                                                                                                                                                         sentence  \n",
       "0                                                                                                   * Decisions about how to do these things are usually made by trial and error.  \n",
       "1  This is usually easily accomplished - in R, for example, there are various different ways of doing it but it's always just one line of code, or a parameter set in a function.  \n",
       "2                                                                                                           Despite the practical ease of achieving this, it still needs thought.  \n",
       "3                                                                                        You can choose to replace the punctuation marks with a space, or remove them altogether.  \n",
       "4                                                           On the other hand, removing punctuation altogether can lead to problems in cases where someone has hyphenated a word.  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in all sentences from our NLP guide (if you lack 'sentences.csv' then run `data_preparation.R`)\n",
    "corpus = pd.read_csv(\"sentences.csv\")\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID</th>\n",
       "      <th>sentence</th>\n",
       "      <th>vectors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>FeatureSelection.md_1</td>\n",
       "      <td>* Decisions about how to do these things are usually made by trial and error.</td>\n",
       "      <td>[-0.0080329105, 0.021076743, 0.0016090235, 0.015615334, 0.021448795, -0.02895572, -0.017227316, -0.16568217, -0.015879735, -0.045628887, -0.027818492, -0.08743536, 0.015253517, -0.01839747, -0.00083485665, 0.021957552, 0.14317317, -0.002015149, 0.10568141, 0.014507441, -0.00872977, 0.017498098, -0.012503439, 0.116140865, -0.048947755, 0.02268431, -0.009576736, 0.0074026776, 0.11766725, -0.012650943, -0.007363485, 0.0065476163, -0.03638299, -0.106922016, 0.036052905, 0.020986998, -0.002827829...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>FeatureSelection.md_2</td>\n",
       "      <td>This is usually easily accomplished - in R, for example, there are various different ways of doing it but it's always just one line of code, or a parameter set in a function.</td>\n",
       "      <td>[0.013527644, -0.012912958, 0.05678807, -0.0023535287, -0.031319316, -0.03373001, 0.0012797157, -0.18278837, -0.038471613, -0.054697327, -0.031186085, -0.09244956, -0.019320857, 0.009983517, -0.02046032, 0.045013547, 0.10419684, 0.03064537, 0.11806083, 0.02091864, 0.0040642424, 0.015343196, -0.010522759, 0.11600212, 0.011793437, 0.023224153, -0.008294441, 0.039412495, 0.08627377, 0.016949164, -0.039776232, -0.01686298, -0.0023626043, -0.10091015, 0.03590936, -0.046496466, -0.01643459, 0.0212...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>FeatureSelection.md_3</td>\n",
       "      <td>Despite the practical ease of achieving this, it still needs thought.</td>\n",
       "      <td>[-0.020053409, -0.0069089155, -0.0060613644, -0.022930084, -0.034667023, -0.008358551, -0.036972728, -0.17744663, -0.033453833, 0.00873448, -0.048085753, -0.0862697, 0.014174514, -0.016834512, -0.0024806417, -0.00915952, 0.1015824, 0.02559705, 0.102265805, 0.033969667, -0.021966875, 0.002373617, 0.06554223, 0.12357432, -0.025039257, -0.012214568, 0.009082507, -0.011551237, 0.09926328, 0.00316169, -0.023433667, -0.017739069, 0.004649915, -0.10267782, 0.021853343, 0.029765343, 0.0019963442, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>FeatureSelection.md_4</td>\n",
       "      <td>You can choose to replace the punctuation marks with a space, or remove them altogether.</td>\n",
       "      <td>[0.03999276, 0.03196642, 0.05058594, 0.005573745, -0.01834618, 0.02007063, 0.028969927, -0.17134686, -0.02729807, -0.054477774, -0.04154321, -0.033670098, -0.018778903, 0.02571134, -0.020630216, 0.043483913, 0.04301242, 0.038266823, 0.10943258, 0.014354443, 0.004463497, 0.00018632211, 0.008865451, 0.10899704, -0.036367856, 0.0109212445, 0.01170619, 0.03975891, 0.025309594, 0.024196852, -0.07005064, -0.0064878585, 0.030491501, -0.03255476, 0.021842735, -0.081583425, -0.00013621786, 0.01781558...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>FeatureSelection.md_5</td>\n",
       "      <td>On the other hand, removing punctuation altogether can lead to problems in cases where someone has hyphenated a word.</td>\n",
       "      <td>[0.0051377737, 0.006850067, 0.021656906, 0.0019045039, -0.021693604, 0.030433074, -0.004772003, -0.18376674, -0.045084, -0.01355956, -0.013582825, -0.04761095, 0.016942099, -0.0065597547, -0.0031950774, 0.026664509, 0.046471883, 0.062809065, 0.094349496, -0.0068967273, 0.012712798, -0.0002191907, 0.020930024, 0.11781901, -0.046308387, 0.012336634, 0.0047920467, 0.07330619, 0.11551752, 0.0053112754, -0.005649141, -0.024537914, 0.018353172, -0.079487145, 0.01152881, -0.06505355, -0.08125301, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                     ID  \\\n",
       "0           1  FeatureSelection.md_1   \n",
       "1           2  FeatureSelection.md_2   \n",
       "2           3  FeatureSelection.md_3   \n",
       "3           4  FeatureSelection.md_4   \n",
       "4           5  FeatureSelection.md_5   \n",
       "\n",
       "                                                                                                                                                                         sentence  \\\n",
       "0                                                                                                   * Decisions about how to do these things are usually made by trial and error.   \n",
       "1  This is usually easily accomplished - in R, for example, there are various different ways of doing it but it's always just one line of code, or a parameter set in a function.   \n",
       "2                                                                                                           Despite the practical ease of achieving this, it still needs thought.   \n",
       "3                                                                                        You can choose to replace the punctuation marks with a space, or remove them altogether.   \n",
       "4                                                           On the other hand, removing punctuation altogether can lead to problems in cases where someone has hyphenated a word.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               vectors  \n",
       "0  [-0.0080329105, 0.021076743, 0.0016090235, 0.015615334, 0.021448795, -0.02895572, -0.017227316, -0.16568217, -0.015879735, -0.045628887, -0.027818492, -0.08743536, 0.015253517, -0.01839747, -0.00083485665, 0.021957552, 0.14317317, -0.002015149, 0.10568141, 0.014507441, -0.00872977, 0.017498098, -0.012503439, 0.116140865, -0.048947755, 0.02268431, -0.009576736, 0.0074026776, 0.11766725, -0.012650943, -0.007363485, 0.0065476163, -0.03638299, -0.106922016, 0.036052905, 0.020986998, -0.002827829...  \n",
       "1  [0.013527644, -0.012912958, 0.05678807, -0.0023535287, -0.031319316, -0.03373001, 0.0012797157, -0.18278837, -0.038471613, -0.054697327, -0.031186085, -0.09244956, -0.019320857, 0.009983517, -0.02046032, 0.045013547, 0.10419684, 0.03064537, 0.11806083, 0.02091864, 0.0040642424, 0.015343196, -0.010522759, 0.11600212, 0.011793437, 0.023224153, -0.008294441, 0.039412495, 0.08627377, 0.016949164, -0.039776232, -0.01686298, -0.0023626043, -0.10091015, 0.03590936, -0.046496466, -0.01643459, 0.0212...  \n",
       "2  [-0.020053409, -0.0069089155, -0.0060613644, -0.022930084, -0.034667023, -0.008358551, -0.036972728, -0.17744663, -0.033453833, 0.00873448, -0.048085753, -0.0862697, 0.014174514, -0.016834512, -0.0024806417, -0.00915952, 0.1015824, 0.02559705, 0.102265805, 0.033969667, -0.021966875, 0.002373617, 0.06554223, 0.12357432, -0.025039257, -0.012214568, 0.009082507, -0.011551237, 0.09926328, 0.00316169, -0.023433667, -0.017739069, 0.004649915, -0.10267782, 0.021853343, 0.029765343, 0.0019963442, -0...  \n",
       "3  [0.03999276, 0.03196642, 0.05058594, 0.005573745, -0.01834618, 0.02007063, 0.028969927, -0.17134686, -0.02729807, -0.054477774, -0.04154321, -0.033670098, -0.018778903, 0.02571134, -0.020630216, 0.043483913, 0.04301242, 0.038266823, 0.10943258, 0.014354443, 0.004463497, 0.00018632211, 0.008865451, 0.10899704, -0.036367856, 0.0109212445, 0.01170619, 0.03975891, 0.025309594, 0.024196852, -0.07005064, -0.0064878585, 0.030491501, -0.03255476, 0.021842735, -0.081583425, -0.00013621786, 0.01781558...  \n",
       "4  [0.0051377737, 0.006850067, 0.021656906, 0.0019045039, -0.021693604, 0.030433074, -0.004772003, -0.18376674, -0.045084, -0.01355956, -0.013582825, -0.04761095, 0.016942099, -0.0065597547, -0.0031950774, 0.026664509, 0.046471883, 0.062809065, 0.094349496, -0.0068967273, 0.012712798, -0.0002191907, 0.020930024, 0.11781901, -0.046308387, 0.012336634, 0.0047920467, 0.07330619, 0.11551752, 0.0053112754, -0.005649141, -0.024537914, 0.018353172, -0.079487145, 0.01152881, -0.06505355, -0.08125301, 0...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embed all the sentences in our corpus\n",
    "corpus['vectors'] = np.array(corpus['sentence'].apply(embed_phrase))\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Glossary.md_21</td>\n",
       "      <td>If you are doing work on *Search* or *Topics*, the *document*s will be the objects which you will be finding similarities between in order to group them topically.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Glossary.md_8</td>\n",
       "      <td>The set of text *document*s that you are analysing.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Glossary.md_17</td>\n",
       "      <td>The vector for a *document* points in the directions of the concepts that *document* contains.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Glossary.md_4</td>\n",
       "      <td>A catch-all term for a group of algorithms that aim to collect *documents* into clusters.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Glossary.md_48</td>\n",
       "      <td>Words routinely removed from *document*s at an early stage of the analysis.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Intro.md_8</td>\n",
       "      <td>Finding parts of the text that are about a particular topic of interest (e.g. allow the user to search for parts of the text that are about biscuits).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>NNmodels.md_31</td>\n",
       "      <td>We can search the parameter (and meta-parameter) space, compute the test results, and use the model that gives us the best answers.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>README.md_2</td>\n",
       "      <td>* [Search through a set of documents](Search.md) * [Find topics in a set of documents](Topics.md) * [Feature Selection](FeatureSelection.md) * [Latent Semantic Analysis (LSA)](LSA.md) * [Latent Dirichlet Allocation (LDA)](LDA.md) * [Word2Vec, Doc2Vec, fastText (Neural Network models)](NNmodels.md) * There is also R code for LSA and LDA accessible in `code/NLP-guidance.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Glossary.md_62</td>\n",
       "      <td>The scheme by which we go from a vector of counts of each word in the *vocabulary* for a given document to an *embedding*.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Glossary.md_56</td>\n",
       "      <td>This function claims that its input must be &gt; ...a document-term matrix ... containing *documents in colums, terms in rows*... (emphasis mine).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Glossary.md_5</td>\n",
       "      <td>The idea is that the documents within each cluster have something in common, and in particular that they have more in common with each other than with documents from outside the cluster.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Intro.md_9</td>\n",
       "      <td>Find key identifiers in the text (e.g. tag all occurrences of people's names, or of names of biscuits).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Glossary.md_1</td>\n",
       "      <td>An approach to *embedding* in which the order of words in a *document* is not considered, just the presence or absence (or sometimes quantity) of terms.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Glossary.md_16</td>\n",
       "      <td>\\text{Similarity&amp;space;of&amp;space;}\\textbf{a}\\text{&amp;space;and&amp;space;}\\textbf{b}&amp;space;=&amp;space;\\cos&amp;space;\\theta\" title=\"\\text{Similarity of }\\textbf{a}\\text{ and }\\textbf{b} = \\cos \\theta\" /&gt;&lt;/a&gt; The rationale for this is that the vector space into which we *embed* our *documents* is defined such that the dimensions in it approximately relate to the concepts within the *documents*.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Glossary.md_32</td>\n",
       "      <td>\\log&amp;space;\\frac{N}{n_t}\" title=\"\\log \\frac{N}{n_t}\" /&gt;&lt;/a&gt; weighting for a word *t*, where *N* is the total number of *documents* in the *corpus*, and *n~t~* is the number of *documents* that contain *t*.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Glossary.md_28</td>\n",
       "      <td>* Counting the number of times a *document* contains the words biscuit or biscuits and assigning this number to the  *document*.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>README.md_1</td>\n",
       "      <td>This repo contains thoughts and guidance about the use of Natural Language Processing, based on the experience of using these techniques in a few projects.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Glossary.md_12</td>\n",
       "      <td>A way of measuring similarity between *documents* after they have been *embedded* as vectors.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>Topics.md_3</td>\n",
       "      <td>* Where topics are derived via an unsupervised machine learning methods their meanings can be hard to define in an appropriate, semantically-meaningful, human-understandable way. * automatically discover what these underlying topics are; and then This question can actually be somewhat more general than just determining the topical content of the text.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Glossary.md_27</td>\n",
       "      <td>The process whereby *documents* or words are coded up as a vector in some (typically very high-dimensional) vector space.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID  \\\n",
       "38   Glossary.md_21   \n",
       "25    Glossary.md_8   \n",
       "34   Glossary.md_17   \n",
       "21    Glossary.md_4   \n",
       "65   Glossary.md_48   \n",
       "92       Intro.md_8   \n",
       "198  NNmodels.md_31   \n",
       "209     README.md_2   \n",
       "79   Glossary.md_62   \n",
       "73   Glossary.md_56   \n",
       "22    Glossary.md_5   \n",
       "93       Intro.md_9   \n",
       "18    Glossary.md_1   \n",
       "33   Glossary.md_16   \n",
       "49   Glossary.md_32   \n",
       "45   Glossary.md_28   \n",
       "208     README.md_1   \n",
       "29   Glossary.md_12   \n",
       "235     Topics.md_3   \n",
       "44   Glossary.md_27   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                           sentence  \n",
       "38                                                                                                                                                                                                                              If you are doing work on *Search* or *Topics*, the *document*s will be the objects which you will be finding similarities between in order to group them topically.  \n",
       "25                                                                                                                                                                                                                                                                                                                                              The set of text *document*s that you are analysing.  \n",
       "34                                                                                                                                                                                                                                                                                                   The vector for a *document* points in the directions of the concepts that *document* contains.  \n",
       "21                                                                                                                                                                                                                                                                                                        A catch-all term for a group of algorithms that aim to collect *documents* into clusters.  \n",
       "65                                                                                                                                                                                                                                                                                                                      Words routinely removed from *document*s at an early stage of the analysis.  \n",
       "92                                                                                                                                                                                                                                           Finding parts of the text that are about a particular topic of interest (e.g. allow the user to search for parts of the text that are about biscuits).  \n",
       "198                                                                                                                                                                                                                                                             We can search the parameter (and meta-parameter) space, compute the test results, and use the model that gives us the best answers.  \n",
       "209             * [Search through a set of documents](Search.md) * [Find topics in a set of documents](Topics.md) * [Feature Selection](FeatureSelection.md) * [Latent Semantic Analysis (LSA)](LSA.md) * [Latent Dirichlet Allocation (LDA)](LDA.md) * [Word2Vec, Doc2Vec, fastText (Neural Network models)](NNmodels.md) * There is also R code for LSA and LDA accessible in `code/NLP-guidance.  \n",
       "79                                                                                                                                                                                                                                                                       The scheme by which we go from a vector of counts of each word in the *vocabulary* for a given document to an *embedding*.  \n",
       "73                                                                                                                                                                                                                                                  This function claims that its input must be > ...a document-term matrix ... containing *documents in colums, terms in rows*... (emphasis mine).  \n",
       "22                                                                                                                                                                                                       The idea is that the documents within each cluster have something in common, and in particular that they have more in common with each other than with documents from outside the cluster.  \n",
       "93                                                                                                                                                                                                                                                                                          Find key identifiers in the text (e.g. tag all occurrences of people's names, or of names of biscuits).  \n",
       "18                                                                                                                                                                                                                                         An approach to *embedding* in which the order of words in a *document* is not considered, just the presence or absence (or sometimes quantity) of terms.  \n",
       "33   \\text{Similarity&space;of&space;}\\textbf{a}\\text{&space;and&space;}\\textbf{b}&space;=&space;\\cos&space;\\theta\" title=\"\\text{Similarity of }\\textbf{a}\\text{ and }\\textbf{b} = \\cos \\theta\" /></a> The rationale for this is that the vector space into which we *embed* our *documents* is defined such that the dimensions in it approximately relate to the concepts within the *documents*.  \n",
       "49                                                                                                                                                                                    \\log&space;\\frac{N}{n_t}\" title=\"\\log \\frac{N}{n_t}\" /></a> weighting for a word *t*, where *N* is the total number of *documents* in the *corpus*, and *n~t~* is the number of *documents* that contain *t*.  \n",
       "45                                                                                                                                                                                                                                                                 * Counting the number of times a *document* contains the words biscuit or biscuits and assigning this number to the  *document*.  \n",
       "208                                                                                                                                                                                                                                     This repo contains thoughts and guidance about the use of Natural Language Processing, based on the experience of using these techniques in a few projects.  \n",
       "29                                                                                                                                                                                                                                                                                                    A way of measuring similarity between *documents* after they have been *embedded* as vectors.  \n",
       "235                               * Where topics are derived via an unsupervised machine learning methods their meanings can be hard to define in an appropriate, semantically-meaningful, human-understandable way. * automatically discover what these underlying topics are; and then This question can actually be somewhat more general than just determining the topical content of the text.  \n",
       "44                                                                                                                                                                                                                                                                        The process whereby *documents* or words are coded up as a vector in some (typically very high-dimensional) vector space.  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we can search\n",
    "search_phrase(\"this is how we can search through our documents\", corpus)[['ID','sentence']][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
