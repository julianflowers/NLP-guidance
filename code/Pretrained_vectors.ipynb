{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first we look at the way we can use pre-trained embeddings using the PyMagnitude package\n",
    "\n",
    "# BEFORE YOU START\n",
    "# You need to have gone to this URL : https://github.com/plasticityai/magnitude#pre-converted-magnitude-formats-of-popular-embeddings-models\n",
    "# downloaded some vectors of you choice (these files can be massive so it might take a while), and stored\n",
    "# them somewhere you can access (e.g. with the file path I specify below).\n",
    "\n",
    "# YOU MAY ALSO\n",
    "# need to have run the data_preparation.R script if you lack the `sentences.csv` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymagnitude import *\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from nltk.corpus import stopwords\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 780 ms, sys: 30.2 ms, total: 810 ms\n",
      "Wall time: 826 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#here's where you get your vectors in - you need to replace the path below with your own one\n",
    "vectors = Magnitude(\"here/you/put/path/to/magnitude_file.magnitude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some things we can do with magnitude vectors for words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8671641"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.distance(\"biscuit\",\"macaroon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8671641, 1.0519347]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.distance(\"biscuit\",[\"macaroon\", \"cabbage\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.62401325"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.similarity(\"biscuit\",\"macaroon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.62401325, 0.4467167]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.similarity(\"biscuit\",[\"macaroon\", \"cabbage\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'macaroon'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.most_similar_to_given(\"biscuit\", [\"macaroon\", \"cabbage\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cabbage'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.doesnt_match([\"biscuit\", \"macaroon\", \"cabbage\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('biscuits', 0.8173666000366211),\n",
       " ('biscuity', 0.6973364353179932),\n",
       " ('cake', 0.6754744052886963),\n",
       " ('chocolate', 0.6582432985305786),\n",
       " ('oatcake', 0.6574128866195679),\n",
       " ('teacake', 0.6451709866523743),\n",
       " ('loaf', 0.6432216167449951),\n",
       " ('cornflake', 0.6331499814987183),\n",
       " ('flapjack', 0.6270235776901245),\n",
       " ('shortbread', 0.6263179779052734)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.most_similar(\"biscuit\", topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('biscuit', 1.0),\n",
       " ('biscuits', 0.8173666000366211),\n",
       " ('biscuity', 0.6973364353179932),\n",
       " ('cake', 0.6754744052886963),\n",
       " ('chocolate', 0.6582432985305786),\n",
       " ('oatcake', 0.6574128866195679),\n",
       " ('teacake', 0.6451709866523743),\n",
       " ('loaf', 0.6432216167449951),\n",
       " ('cornflake', 0.6331499814987183),\n",
       " ('flapjack', 0.6270235776901245)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.most_similar(vectors.query(\"biscuit\"), topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Berlin', 0.7935939431190491),\n",
       " ('Munich', 0.7534011006355286),\n",
       " ('Frankfurt', 0.7376378774642944),\n",
       " ('Cologne', 0.7260650992393494),\n",
       " ('Stuttgart', 0.7239525318145752),\n",
       " ('Leipzig', 0.7191416025161743),\n",
       " ('Vienna', 0.7057973146438599),\n",
       " ('Hamburg', 0.7021979093551636),\n",
       " ('Frankfurt-am-Main', 0.6996399164199829),\n",
       " ('DÃ¼sseldorf', 0.6979357004165649)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.most_similar(positive=['Paris', 'Germany'], negative=['France'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Iraq', 0.7341831922531128),\n",
       " ('Kuwait', 0.6448562145233154),\n",
       " ('Mosul', 0.6440438628196716),\n",
       " ('Basra', 0.6172256469726562),\n",
       " ('Iraq--and', 0.6111494302749634),\n",
       " ('Al-Anbar', 0.6111178398132324),\n",
       " ('Baghdady', 0.6097682118415833),\n",
       " ('Al-Basrah', 0.6095267534255981),\n",
       " ('Al-Najaf', 0.6082150936126709),\n",
       " ('Basrah', 0.6069812774658203)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.most_similar(positive=['England', 'Baghdad'], negative=['London'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This embeds a search phrase or report sentence in our 300-dimensional vector space\n",
    "#by simply averaging over the words in the phrase\n",
    "# I have no doubt there are better ways of doing this.\n",
    "def embed_phrase(phrase):\n",
    "    try:\n",
    "        input_nopunc = re.sub(r'[^\\w\\s]','',phrase) #take out punctuation\n",
    "        input_lower = input_nopunc.lower().split() #make lower case and split by word\n",
    "        #now take out words not in word2vec model, and also words in nltk stopword list\n",
    "        clean_search = [word for word in input_lower if (word in vectors and word not in stopwords.words(\"English\"))]\n",
    "        if clean_search: #this means if clean_search isn't empty\n",
    "            unnorm_vector = np.mean(vectors.query(clean_search), axis = 0) #take mean of vectors of words that remain\n",
    "        else:\n",
    "            unnorm_vector = np.zeros(300) #set to zero if no words remain\n",
    "    except(TypeError):\n",
    "        unnorm_vector = np.zeros(300) #set to zero if it doesn't seem to be a string\n",
    "    #we normalize the result to length 1 so we can use dot products for cosine similarity\n",
    "    norm_array = preprocessing.normalize(unnorm_vector.reshape(-1,1), norm = 'l2', axis = 0)\n",
    "    return(np.concatenate(norm_array))\n",
    "\n",
    "#this allows you to search a phrase and compare it to a set of comparison sentences\n",
    "# again, no doubt this could be greatly improved.\n",
    "def search_phrase(phrase, comparison_set):\n",
    "    results_df = comparison_set\n",
    "    #we take our search phrase and compute its dot product with all of our guide sentences\n",
    "    #then we reorder by how similar the phrase is (larger dot product = more similar)\n",
    "    #and give the top 10\n",
    "    embedding = np.array(results_df['vectors'].values.tolist()).T\n",
    "    results_df['search_results'] = np.dot(embed_phrase(phrase), embedding)\n",
    "    return(results_df.sort_values(by=['search_results'])[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.28026947e-03, -2.43652239e-02,  1.32176944e-03, -9.69787687e-03,\n",
       "       -3.74229029e-02, -8.92745480e-02, -2.56897439e-03, -1.03370681e-01,\n",
       "       -6.57415316e-02,  1.29279522e-02,  8.24017625e-04,  7.51992390e-02,\n",
       "        2.91189346e-02, -2.35687736e-02,  5.70440330e-02,  4.56786007e-02,\n",
       "        1.60301358e-01,  1.96943642e-03,  1.33410349e-01,  3.72173935e-02,\n",
       "        2.48401333e-03,  7.24656135e-02,  6.67128386e-03,  4.35236953e-02,\n",
       "        3.62036712e-02, -1.57978833e-02,  5.85241467e-02, -2.75252834e-02,\n",
       "        4.37867455e-02, -3.61571088e-02,  1.13285063e-02, -2.19578911e-02,\n",
       "       -1.92831922e-02,  7.66812358e-03,  3.62293907e-02, -5.31622693e-02,\n",
       "        9.04731266e-03, -8.52390379e-03,  1.10962684e-03,  4.68735360e-02,\n",
       "       -2.17354670e-02, -1.00070961e-01, -9.81302857e-02, -9.39529669e-03,\n",
       "        4.40335227e-03, -2.58727856e-02,  1.92488786e-02, -5.05256131e-02,\n",
       "       -1.49174258e-02,  3.19914962e-03, -1.35877803e-02,  1.67603744e-03,\n",
       "        2.21653134e-02, -5.13803810e-02, -7.54389465e-02, -3.97904776e-02,\n",
       "        1.56661253e-02, -6.39626160e-02, -1.44768968e-01, -8.69245231e-02,\n",
       "       -4.93399501e-02,  8.51072930e-03,  2.11666003e-01, -1.15942806e-02,\n",
       "        2.94910986e-02,  5.58069870e-02, -6.09389693e-03,  6.76216409e-02,\n",
       "       -4.64824662e-02, -8.43283311e-02,  2.86841057e-02, -2.87931319e-02,\n",
       "        6.60953447e-02,  1.33042841e-03, -1.73678454e-02, -2.09496748e-02,\n",
       "       -3.32476236e-02, -6.30467385e-02, -3.38993967e-02,  1.26243476e-03,\n",
       "       -3.29608545e-02, -1.71698648e-02,  3.81438434e-03,  1.32885426e-01,\n",
       "        4.13149921e-03, -1.16397161e-02,  2.13067164e-03, -9.64173174e-04,\n",
       "       -3.78217995e-02, -5.15193539e-03, -1.27339931e-02,  3.69886868e-02,\n",
       "       -1.67766064e-01, -4.71423827e-02,  1.08763136e-01,  6.94216117e-02,\n",
       "       -1.92770828e-02,  6.03497438e-02,  4.98811640e-02, -1.28916102e-02,\n",
       "       -5.31006716e-02,  3.61709826e-04, -3.99939390e-03,  1.44440448e-04,\n",
       "       -4.62693162e-02, -2.20617995e-01,  8.78921431e-03,  4.21808437e-02,\n",
       "        3.15021463e-02,  6.52092919e-02, -5.82377575e-02,  4.46241423e-02,\n",
       "        7.93585926e-02,  5.54850176e-02,  6.37287647e-03,  2.38732081e-02,\n",
       "       -4.28756773e-02, -2.68710982e-02, -7.00625926e-02, -3.43196653e-02,\n",
       "       -1.28169870e-02,  1.85127445e-02, -3.57695706e-02, -1.83183029e-01,\n",
       "        9.78112817e-02,  1.13108335e-03,  1.03419209e-02, -9.19226855e-02,\n",
       "       -5.00338003e-02,  5.76336123e-02, -2.05343366e-02,  6.49174824e-02,\n",
       "       -3.22946906e-03, -9.54657141e-03, -7.79695883e-02, -7.16035441e-02,\n",
       "       -2.48756614e-02,  9.27762464e-02, -1.30076418e-02,  2.20725909e-02,\n",
       "       -5.79383830e-03, -3.87883745e-02, -6.07131096e-03, -2.41142195e-02,\n",
       "        4.79082577e-02,  6.71027154e-02,  2.08626837e-02,  1.30466782e-02,\n",
       "       -2.22602654e-02,  9.72489268e-03,  1.69801060e-02,  9.70375910e-02,\n",
       "        3.32811587e-02, -6.63768947e-02,  6.26092544e-03, -8.06729794e-02,\n",
       "       -7.13220378e-03,  3.24521549e-02, -2.39890907e-02,  3.40269343e-03,\n",
       "        2.01487038e-02, -3.38619575e-02, -2.90569151e-03,  2.04770565e-02,\n",
       "       -2.32404210e-02, -1.70471426e-02, -2.25862432e-02, -1.72944423e-02,\n",
       "        7.62891071e-03,  5.39487824e-02, -1.49774575e-03,  2.46082935e-02,\n",
       "       -2.06573159e-02, -4.53929082e-02,  2.88893580e-02,  1.20459981e-01,\n",
       "       -2.47964319e-02, -1.84397735e-02,  4.23969061e-04, -3.80148888e-02,\n",
       "        1.08625740e-01, -3.47708799e-02,  1.17191614e-03, -1.59638256e-01,\n",
       "        4.95294444e-02, -3.73813510e-02,  1.52369728e-02,  2.26802945e-01,\n",
       "       -3.20027918e-02,  1.16243083e-02,  4.11033034e-02,  1.59276262e-01,\n",
       "       -5.26110567e-02,  7.38882518e-04, -4.41474989e-02,  9.05307010e-02,\n",
       "        1.91506818e-02, -7.96159059e-02, -7.12293480e-03,  5.96217513e-02,\n",
       "       -1.63366199e-01,  1.04357980e-01,  7.10449517e-02,  6.39446005e-02,\n",
       "       -2.70788986e-02, -3.44649479e-02,  6.69798348e-03, -3.22976112e-02,\n",
       "        2.73473375e-02, -6.83618486e-02,  1.26708031e-01, -2.41828486e-02,\n",
       "       -2.18148120e-02, -3.37664001e-02,  7.89808482e-02,  2.38013342e-02,\n",
       "       -2.09949333e-02,  3.23087946e-02, -6.13101274e-02, -6.37964830e-02,\n",
       "        8.09582248e-02, -1.69871119e-03,  1.21889254e-02,  1.32810473e-01,\n",
       "        5.57901971e-02, -7.57667189e-03,  4.91595715e-02,  1.21370472e-01,\n",
       "        5.08492719e-03, -2.11056992e-02, -6.61255838e-03, -1.05227031e-01,\n",
       "       -1.08702555e-01,  9.79841687e-03,  4.95860241e-02,  4.61855717e-02,\n",
       "       -2.53480747e-02, -3.37899998e-02,  1.55376112e-02,  8.48953053e-03,\n",
       "        1.17619656e-01, -4.75333408e-02, -9.34155099e-03, -3.92557532e-02,\n",
       "       -9.75808054e-02,  4.80091684e-02,  5.93564333e-03, -5.14188828e-03,\n",
       "       -4.01025526e-02, -2.62064636e-02, -2.67107598e-02, -6.65480420e-02,\n",
       "        3.29647632e-03,  3.16362493e-02,  1.04835257e-03,  2.84470301e-02,\n",
       "       -4.33929525e-02,  1.20537532e-02,  1.95015315e-02, -1.81908754e-03,\n",
       "        1.67649221e-02,  5.30064739e-02,  3.38253565e-02, -2.40331963e-02,\n",
       "        1.09972861e-02,  6.03563478e-03, -6.19689710e-02, -1.84885953e-02,\n",
       "        3.99849564e-02, -1.81474432e-03,  4.29078862e-02,  1.45504668e-01,\n",
       "       -1.33573025e-01, -4.40021344e-02,  8.69530812e-02, -7.70415813e-02,\n",
       "        4.54186201e-02, -3.87175381e-02, -5.08772507e-02, -2.71983463e-02,\n",
       "        1.43552702e-02,  3.82375158e-02, -1.50599573e-02,  5.37734106e-03,\n",
       "       -2.43284758e-02,  2.10986882e-02, -2.35810224e-02, -4.17609029e-02,\n",
       "       -2.41251942e-02,  4.49815914e-02, -4.18738686e-02, -1.94134712e-02,\n",
       "       -5.25510907e-02, -1.98369175e-02,  1.62952021e-02, -2.05307137e-02,\n",
       "        2.14534793e-02,  4.20493484e-02,  4.64555435e-03, -2.45881942e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_phrase(\"jaffa cakes are my favourite and I like them the best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>FeatureSelection.md_1</td>\n",
       "      <td>* Decisions about how to do these things are u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>FeatureSelection.md_2</td>\n",
       "      <td>This is usually easily accomplished - in R, fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>FeatureSelection.md_3</td>\n",
       "      <td>Despite the practical ease of achieving this, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>FeatureSelection.md_4</td>\n",
       "      <td>You can choose to replace the punctuation mark...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>FeatureSelection.md_5</td>\n",
       "      <td>On the other hand, removing punctuation altoge...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                     ID  \\\n",
       "0           1  FeatureSelection.md_1   \n",
       "1           2  FeatureSelection.md_2   \n",
       "2           3  FeatureSelection.md_3   \n",
       "3           4  FeatureSelection.md_4   \n",
       "4           5  FeatureSelection.md_5   \n",
       "\n",
       "                                            sentence  \n",
       "0  * Decisions about how to do these things are u...  \n",
       "1  This is usually easily accomplished - in R, fo...  \n",
       "2  Despite the practical ease of achieving this, ...  \n",
       "3  You can choose to replace the punctuation mark...  \n",
       "4  On the other hand, removing punctuation altoge...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = pd.read_csv(\"sentences.csv\")\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus['vectors'] = np.array(corpus['sentence'].apply(embed_phrase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID</th>\n",
       "      <th>sentence</th>\n",
       "      <th>vectors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>FeatureSelection.md_1</td>\n",
       "      <td>* Decisions about how to do these things are u...</td>\n",
       "      <td>[-0.0080329105, 0.021076743, 0.0016090235, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>FeatureSelection.md_2</td>\n",
       "      <td>This is usually easily accomplished - in R, fo...</td>\n",
       "      <td>[0.013527644, -0.012912958, 0.05678807, -0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>FeatureSelection.md_3</td>\n",
       "      <td>Despite the practical ease of achieving this, ...</td>\n",
       "      <td>[-0.020053409, -0.0069089155, -0.0060613644, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>FeatureSelection.md_4</td>\n",
       "      <td>You can choose to replace the punctuation mark...</td>\n",
       "      <td>[0.03999276, 0.03196642, 0.05058594, 0.0055737...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>FeatureSelection.md_5</td>\n",
       "      <td>On the other hand, removing punctuation altoge...</td>\n",
       "      <td>[0.0051377737, 0.006850067, 0.021656906, 0.001...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                     ID  \\\n",
       "0           1  FeatureSelection.md_1   \n",
       "1           2  FeatureSelection.md_2   \n",
       "2           3  FeatureSelection.md_3   \n",
       "3           4  FeatureSelection.md_4   \n",
       "4           5  FeatureSelection.md_5   \n",
       "\n",
       "                                            sentence  \\\n",
       "0  * Decisions about how to do these things are u...   \n",
       "1  This is usually easily accomplished - in R, fo...   \n",
       "2  Despite the practical ease of achieving this, ...   \n",
       "3  You can choose to replace the punctuation mark...   \n",
       "4  On the other hand, removing punctuation altoge...   \n",
       "\n",
       "                                             vectors  \n",
       "0  [-0.0080329105, 0.021076743, 0.0016090235, 0.0...  \n",
       "1  [0.013527644, -0.012912958, 0.05678807, -0.002...  \n",
       "2  [-0.020053409, -0.0069089155, -0.0060613644, -...  \n",
       "3  [0.03999276, 0.03196642, 0.05058594, 0.0055737...  \n",
       "4  [0.0051377737, 0.006850067, 0.021656906, 0.001...  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>LDA.md_1</td>\n",
       "      <td>* Latent Dirichlet Allocation is a probabilistic method for [*Topic*](Topics.md) Modelling.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>TLDR.md_9</td>\n",
       "      <td>* Latent Dirichlet Allocation is a probabilistic method for [*Topic*](Topics.md) Modelling.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Glossary.md_29</td>\n",
       "      <td>* Latent semantic analysis.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Glossary.md_52</td>\n",
       "      <td>Used in [*Latent Semantic Analysis*](LSA.md).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>README.md_2</td>\n",
       "      <td>* [Search through a set of documents](Search.md) * [Find topics in a set of documents](Topics.md) * [Feature Selection](FeatureSelection.md) * [Latent Semantic Analysis (LSA)](LSA.md) * [Latent Dirichlet Allocation (LDA)](LDA.md) * [Word2Vec, Doc2Vec, fastText (Neural Network models)](NNmodels.md) * There is also R code for LSA and LDA accessible in `code/NLP-guidance.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>TLDR.md_1</td>\n",
       "      <td>* Following [feature selection](FeatureSelection.md), some methodologies we've had success with are [Latent Semantic Analysis](LSA.md) and [Neural Network models like Word2Vec and Doc2Vec](NNmodels.md).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>Search.md_1</td>\n",
       "      <td>* Following [feature selection](FeatureSelection.md), some methodologies we've had success with are [Latent Semantic Analysis](LSA.md) and [Neural Network models like Word2Vec and Doc2Vec](NNmodels.md).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>LDA.md_21</td>\n",
       "      <td>All dimensionality reduction type approaches in natural language processing suffer from this to some extent, but the complexity of LDA makes it worse here in my opinion.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>LSA.md_28</td>\n",
       "      <td>Finally, note that in terms of being an algorithm that * uses matrix algebra; and * results in the creation of a reduced-rank subspace; which * is somehow closest to the original space; and * has a basis that is a linear combination of the original basis vectors SVD is clearly similar in ethos to principal component analysis.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Glossary.md_6</td>\n",
       "      <td>Clustering algorithms typically require some measure of distance (or, to some extent equivalently, similarity) between *documents* in a vector space.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>Topics.md_2</td>\n",
       "      <td>* Latent Dirichlet Allocation is often mentioned in connection with this; we've never got it to work.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>TLDR.md_4</td>\n",
       "      <td>* Latent Dirichlet Allocation is often mentioned in connection with this; we've never got it to work.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>NNmodels.md_19</td>\n",
       "      <td>The major distinction between CBOW and skip-gram models is in the task that the neural network is given.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Glossary.md_53</td>\n",
       "      <td>Defines a subspace of our initial vector space, the rank of which is the smaller out of the number of documents and the size of the vocabulary.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>README.md_1</td>\n",
       "      <td>This repo contains thoughts and guidance about the use of Natural Language Processing, based on the experience of using these techniques in a few projects.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Glossary.md_63</td>\n",
       "      <td>Typically made up of three elements: term frequency, *inverse document frequency*, and *normalisation*.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>TLDR.md_5</td>\n",
       "      <td>* For the unsupervised machine learning techniques, typically you have to define some parameter such as the number of topics, or the density of points in your vector space to define a cluster.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>FeatureSelection.md_15</td>\n",
       "      <td>We may not want this granularity of information in natural language processing projects.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>NNmodels.md_34</td>\n",
       "      <td>The above two tests are more important because they capture some element of the actual use case of our model.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>TLDR.md_8</td>\n",
       "      <td>Although there are doubtless statistical measures for how \"good\" a value of *k* is (along the lines of plotting some loss of information and looking for an elbow in the plot), we have instead focussed on investigating what happens in the actual tool.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         ID  \\\n",
       "96                 LDA.md_1   \n",
       "229               TLDR.md_9   \n",
       "46           Glossary.md_29   \n",
       "69           Glossary.md_52   \n",
       "209             README.md_2   \n",
       "221               TLDR.md_1   \n",
       "211             Search.md_1   \n",
       "116               LDA.md_21   \n",
       "148               LSA.md_28   \n",
       "23            Glossary.md_6   \n",
       "234             Topics.md_2   \n",
       "224               TLDR.md_4   \n",
       "186          NNmodels.md_19   \n",
       "70           Glossary.md_53   \n",
       "208             README.md_1   \n",
       "80           Glossary.md_63   \n",
       "225               TLDR.md_5   \n",
       "14   FeatureSelection.md_15   \n",
       "201          NNmodels.md_34   \n",
       "228               TLDR.md_8   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                sentence  \n",
       "96                                                                                                                                                                                                                                                                                           * Latent Dirichlet Allocation is a probabilistic method for [*Topic*](Topics.md) Modelling.  \n",
       "229                                                                                                                                                                                                                                                                                          * Latent Dirichlet Allocation is a probabilistic method for [*Topic*](Topics.md) Modelling.  \n",
       "46                                                                                                                                                                                                                                                                                                                                                           * Latent semantic analysis.  \n",
       "69                                                                                                                                                                                                                                                                                                                                         Used in [*Latent Semantic Analysis*](LSA.md).  \n",
       "209  * [Search through a set of documents](Search.md) * [Find topics in a set of documents](Topics.md) * [Feature Selection](FeatureSelection.md) * [Latent Semantic Analysis (LSA)](LSA.md) * [Latent Dirichlet Allocation (LDA)](LDA.md) * [Word2Vec, Doc2Vec, fastText (Neural Network models)](NNmodels.md) * There is also R code for LSA and LDA accessible in `code/NLP-guidance.  \n",
       "221                                                                                                                                                                           * Following [feature selection](FeatureSelection.md), some methodologies we've had success with are [Latent Semantic Analysis](LSA.md) and [Neural Network models like Word2Vec and Doc2Vec](NNmodels.md).  \n",
       "211                                                                                                                                                                           * Following [feature selection](FeatureSelection.md), some methodologies we've had success with are [Latent Semantic Analysis](LSA.md) and [Neural Network models like Word2Vec and Doc2Vec](NNmodels.md).  \n",
       "116                                                                                                                                                                                                            All dimensionality reduction type approaches in natural language processing suffer from this to some extent, but the complexity of LDA makes it worse here in my opinion.  \n",
       "148                                              Finally, note that in terms of being an algorithm that * uses matrix algebra; and * results in the creation of a reduced-rank subspace; which * is somehow closest to the original space; and * has a basis that is a linear combination of the original basis vectors SVD is clearly similar in ethos to principal component analysis.  \n",
       "23                                                                                                                                                                                                                                 Clustering algorithms typically require some measure of distance (or, to some extent equivalently, similarity) between *documents* in a vector space.  \n",
       "234                                                                                                                                                                                                                                                                                * Latent Dirichlet Allocation is often mentioned in connection with this; we've never got it to work.  \n",
       "224                                                                                                                                                                                                                                                                                * Latent Dirichlet Allocation is often mentioned in connection with this; we've never got it to work.  \n",
       "186                                                                                                                                                                                                                                                                             The major distinction between CBOW and skip-gram models is in the task that the neural network is given.  \n",
       "70                                                                                                                                                                                                                                       Defines a subspace of our initial vector space, the rank of which is the smaller out of the number of documents and the size of the vocabulary.  \n",
       "208                                                                                                                                                                                                                          This repo contains thoughts and guidance about the use of Natural Language Processing, based on the experience of using these techniques in a few projects.  \n",
       "80                                                                                                                                                                                                                                                                               Typically made up of three elements: term frequency, *inverse document frequency*, and *normalisation*.  \n",
       "225                                                                                                                                                                                     * For the unsupervised machine learning techniques, typically you have to define some parameter such as the number of topics, or the density of points in your vector space to define a cluster.  \n",
       "14                                                                                                                                                                                                                                                                                              We may not want this granularity of information in natural language processing projects.  \n",
       "201                                                                                                                                                                                                                                                                        The above two tests are more important because they capture some element of the actual use case of our model.  \n",
       "228                                                                                                                           Although there are doubtless statistical measures for how \"good\" a value of *k* is (along the lines of plotting some loss of information and looking for an elbow in the plot), we have instead focussed on investigating what happens in the actual tool.  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_phrase(\"latent dirichlet allocation\", corpus)[['ID','sentence']][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
